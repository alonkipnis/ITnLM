{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alex Tsvetkov\n",
    "<br>\n",
    "\n",
    "### Final project in the class: Information-theoretic analysis of neural language models\n",
    "#### Recihman University, Fall 2022-2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords Extraction from Emails via Entropy Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation for Keywords/Key-phrases extraction within emails:\n",
    "- Email traffic is really high and it would be helpful to get a general idea of what the email is about without going through it.\n",
    "- Key phrases can offer an easy form of summarization - they convey the main information within the email.\n",
    "- Key phrases and keywords can be used later on to filter emails into categories or for retrieval of similar emails based on keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "The quality of the results during the workshop are incremental i.e. - they will improve as it progresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing methods for keywords extraction\n",
    "- RAKE: partitions documents by stop-words and word/phrase delimiters into candidate keywords. \n",
    "Then computes co-occurrence matrix and based on the matrix gives a score for each word(deg/freq) - the it selects top n scores of summed sequences.\n",
    "Overall efficient and light. \n",
    "https://www.researchgate.net/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents\n",
    "\n",
    "- YAKE!: \n",
    "A light-weight unsupervised automatic keyword extraction method which rests on text statistical features extracted from single documents to select the most important keywords of a text.\n",
    "\n",
    "- TF-IDF - self-explanatory.\n",
    "\n",
    "- Key-Bert computes sentence embedding and then finds ngrams which has the highest cosine similarity to the sentence embedding.\n",
    "\n",
    "- Key-Bert with POS pre matching - same, but instead od specifying ngrams you specify POS matchers for phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AESLC -  Annotated Enron Subject Line Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "!python -m spacy download en_core_web_sm\n",
    "%pip install datasets\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset_builder\n",
    "ds_builder = load_dataset_builder(\"aeslc\")\n",
    "\n",
    "ds_builder.info.description\n",
    "\n",
    "ds_builder.info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"aeslc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example for keywords from email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random  sample from the training set\n",
    "import random\n",
    "# for i in random.sample(range(0, len(dataset[\"train\"])), 3):\n",
    "#     print(dataset[\"train\"][i][\"email_body\"])\n",
    "\n",
    "print(dataset[\"train\"][0][\"email_body\"])\n",
    "print(\"Possible Keywords:\")\n",
    "print(dataset[\"train\"][0][\"subject_line\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2\n",
    "\n",
    "- Trained on WebText Dataset: filtered outbound links from reddit which received at least 3 karma( heuristic indicator for whether other users found the link interesting, educational,or just funny).\n",
    "- 355M parameters, similar largest original BERT (GPT-3 in comparison 175 BN parameters)\n",
    "- Byte Pair encoding tokenizer (vocab size of 50257) - https://towardsdatascience.com/comparing-transformer-tokenizers-686307856955\n",
    "- Trained on 512 batch size\n",
    "- Input size 1024 tokens\n",
    "- Similar to original GPT and any decoder Transformer in architecture.\n",
    "- Can be used to evaluate entropy since it's auto-regressive - attends to past (similar to information flow)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Card: https://huggingface.co/gpt2-medium\n",
    "- Paper: https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "\n",
    "# This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n",
    "# be encoded differently whether it is at the beginning of the sentence (without space) or not\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2-medium')\n",
    "\n",
    "# 355M parameter version of GPT-2 which was trained with language modeling objective\n",
    "# Has a language modeling head on top\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity of auto-regressive LLMs:\n",
    "\n",
    "- GPT2 has been trained with minimization of cross entropy loss on next token prediction(which is the as same max likelihood)\n",
    "- This can be viewed as training on minimizing the perplexity/cross entropy - i.e. predicting the next most probable token /encoding the language in the most efficient manner\n",
    "- Perplexity score(base 2) - confusion of the model, e.g perplexity 10 means that the model is confident in the word as if it had to choose it uniformly from 10 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average word entropy per email based on email length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_body(email_body, tokenizer, is_batched=False):\n",
    "    if is_batched:\n",
    "        email_body = [normalize_body(email, tokenizer, is_batched=False) for email in email_body]\n",
    "    else:\n",
    "        email_body = email_body.replace(\"\\t\", \" \")\n",
    "        # Adding bos token to the beginning of each email\n",
    "        email_body = tokenizer.bos_token + \" \".join(email_body.split())\n",
    "\n",
    "    return email_body\n",
    "\n",
    "\n",
    "def tokenize_emails(dataset, tokenizer, should_use_padding=True, should_only_normalize=False):\n",
    "    dataset = dataset.map(\n",
    "        lambda batch: {\n",
    "            \"email_body\": normalize_body(batch[\"email_body\"], tokenizer, is_batched=True)\n",
    "        },\n",
    "        batched=True,\n",
    "    )\n",
    "    if should_only_normalize:\n",
    "        return dataset\n",
    "    dataset.set_format(type=\"torch\")\n",
    "    # Set a padding token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Batch tokenize the dataset\n",
    "    tokenized_emails = dataset.map(\n",
    "        lambda x: {\n",
    "            **tokenizer(\n",
    "                x[\"email_body\"],\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=\"max_length\" if should_use_padding else False,\n",
    "                add_special_tokens=True,\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # set format of the dataset to pytorch tensors\n",
    "\n",
    "    return tokenized_emails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_emails_non_padded = tokenize_emails(dataset[\"train\"], tokenizer, should_use_padding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute percentiles of tokenized emails\n",
    "import numpy as np\n",
    "\n",
    "length_dataset = tokenized_emails_non_padded.map(lambda x: {\"len\" :len(x[\"input_ids\"][0])})\n",
    "# tokenized_emails_lengths = [len(tokenized_emails_non_padded[\"input_ids\"][0]) for tokenized_email in tokenized_emails_non_padded]\n",
    "print(\" 95th percentile of token length of tokenized emails: \")\n",
    "# print(np.percentile(tokenized_emails_lengths, 95))\n",
    "print(np.percentile(length_dataset[\"len\"], 95))\n",
    "# Average length of tokenized emails\n",
    "print(\"Median of tokenized emails: \")\n",
    "# print(np.mean(tokenized_emails_lengths))\n",
    "print(np.percentile(length_dataset[\"len\"], 50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of tokenized emails length\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_distribution(dataset, title):\n",
    "    plt.hist(dataset[\"len\"], bins=100)\n",
    "    plt.title(title)\n",
    "    # Set y axis label\n",
    "    plt.ylabel(\"Number of emails\")\n",
    "    # Set x axis label\n",
    "    plt.xlabel(\"Length of tokenized emails\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_distribution(length_dataset, \"Distribution of tokenized emails length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train = tokenize_emails(dataset[\"train\"], tokenizer, should_only_normalize=False, should_use_padding=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It seems that average entropy per token is going down as emails are longer\n",
    "- Context of longer emails gives the model more confidence on predictions.\n",
    "- Larger emails contain more redundancy and can be encoded more efficiently on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy_from_logits(logits , relevant_tokens ):\n",
    "    probs = logits[0].softmax(dim=-1)\n",
    "\n",
    "    # Get the actual probs of each token\n",
    "    actual_probs = probs[range(len(probs) -1), relevant_tokens]\n",
    "\n",
    "    # Get cross entropy of each token\n",
    "    entropy = -actual_probs.log2()\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_entropy(model, tokenized_email):\n",
    "\n",
    "    # Convert model to cpu\n",
    "    model.cpu()\n",
    "    # Get probabilities from logits\n",
    "    logits = model(**tokenized_email).logits\n",
    "\n",
    "    # Remove bos token from tokenized email\n",
    "    relevant_tokens = tokenized_email[\"input_ids\"][0][1:]\n",
    "\n",
    "    entropy = calculate_entropy_from_logits(logits, relevant_tokens)\n",
    "\n",
    "    return entropy , relevant_tokens, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_email():\n",
    "    email = dataset[\"train\"][random.randint(0, len(dataset[\"train\"]))]\n",
    "    email_body = email[\"email_body\"]\n",
    "    email_body = email_body.replace(\"\\t\", \" \")\n",
    "    email_body = \" \".join(email_body.split())\n",
    "    return email_body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_single_email(email_body, tokenizer):\n",
    "    tokenized_email = tokenizer(\n",
    "        tokenizer.bos_token + email_body,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    return tokenized_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_entropy_tokens(entropy, relevant_tokens, n):\n",
    "    # Select the n tokens with the highest entropy\n",
    "    top_n_elements = entropy.topk(n, dim=-1).indices\n",
    "\n",
    "    # Take first n elements\n",
    "    top_n_elements = top_n_elements[:n]\n",
    "\n",
    "    # Select these tokens from the original tokenized email\n",
    "    top_n_tokens = relevant_tokens[top_n_elements]\n",
    "\n",
    "    return top_n_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_and_print_tokens_with_entropy(relevant_tokens, entropy, should_display_entropy=True):\n",
    "    decoded = tokenizer.batch_decode(relevant_tokens)\n",
    "\n",
    "    if should_display_entropy:\n",
    "        token_entropy = list(zip(decoded, entropy.tolist(), range(len(relevant_tokens))))\n",
    "    else :\n",
    "        token_entropy = list(zip(decoded, range(len(relevant_tokens))))\n",
    "\n",
    "    print(token_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_indices(some_list, k):\n",
    "    # Convert to numpy array\n",
    "    some_list = np.array(some_list)\n",
    "    # Get the indices of the top k values\n",
    "    top_k_indices = some_list.argsort()[-k:][::-1]\n",
    "    # return as list\n",
    "    return top_k_indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_pairs_of_highest_entropy(entropy, decoded, k, n):\n",
    "    entropy_pairs = []\n",
    "    entropy_pairs_tokens = []\n",
    "    # Fill tensor with average entropy of each k pair\n",
    "    for i in range(len(entropy) - n):\n",
    "        entropy_pairs.append(entropy[i:i+n].mean().detach().numpy())\n",
    "        entropy_pairs_tokens.append(decoded[i:i+n])\n",
    "\n",
    "    # Choose the n pairs with the highest average entropy\n",
    "    top_n_pairs = get_top_k_indices(entropy_pairs, k)\n",
    "\n",
    "    # Get original \n",
    "    top_n_pairs_tokens = [entropy_pairs_tokens[i] for i in top_n_pairs]\n",
    "\n",
    "    return top_n_pairs_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get keywords directly from Email By Cross-Entropy(log loss)\n",
    "- Tokenize email\n",
    "- Calculate cross entropy of each token(which is an upper bound to the real entropy)\n",
    "- Select the n words with the highest entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_body =sample_email()\n",
    "print(email_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the email with  bos token at the beginning and eos token at the end\n",
    "tokenized_email = tokenize_single_email(email_body, tokenizer)\n",
    "\n",
    "num_of_tokens = len(tokenized_email[\"input_ids\"][0])\n",
    "print(f\"Number of tokens in the email: {num_of_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log cross Entropy(log loss) of original email by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "should_display_entropy = True\n",
    "\n",
    "entropy , relevant_tokens, logits = get_tokens_entropy(model, tokenized_email)\n",
    "\n",
    "decode_and_print_tokens_with_entropy(relevant_tokens, entropy, should_display_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get argmax of the logits\n",
    "argmax = logits.argmax(dim=-1)\n",
    "\n",
    "\n",
    "# Decode these tokens\n",
    "actual_tokens_decoded = tokenizer.batch_decode(argmax[0])\n",
    "\n",
    "# Print the actual tokens\n",
    "print(list(zip(actual_tokens_decoded, range(len(actual_tokens_decoded)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print email body\n",
    "print(email_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram length\n",
    "n = 10\n",
    "# number of pairs to print \n",
    "k = 1\n",
    "decoded = tokenizer.batch_decode(relevant_tokens)\n",
    "print(\"Pairs of tokens with the highest entropy:\")\n",
    "print(get_k_pairs_of_highest_entropy(entropy, decoded, k, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting key-phrases with maximal entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram length\n",
    "n = 3\n",
    "# number of pairs to print \n",
    "k = 3\n",
    "decoded = tokenizer.batch_decode(relevant_tokens)\n",
    "print(\"Pairs of tokens with the highest entropy:\")\n",
    "get_k_pairs_of_highest_entropy(entropy, decoded, k, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use KeyBert to extract keywords\n",
    "- document embeddings are extracted with BERT to get a document-level representation.  word embeddings are extracted for N-gram words/phrases.use cosine similarity to find the words/phrases that are the most similar to the document. The most similar words could then be identified as the words that best describe the entire document.\n",
    "\n",
    "- Keybert github: https://github.com/MaartenGr/KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import keybert\n",
    "from keybert import KeyBERT\n",
    "\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(email_body, keyphrase_ngram_range=(1, 1), stop_words=None)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = kw_model.extract_keywords(email_body, keyphrase_ngram_range=(1, n), stop_words=None)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanations for results:\n",
    "\n",
    " - Early token curse(most of the perplexity is assigned to the early tokens due to lack of context)\n",
    " - Distribution shift of email domain - higher perplexity score on email specific terms.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salient Sentences via average cross-entropy(log loss) of sentence\n",
    "\n",
    "## Motivation\n",
    "- Sentences with high cross-entropy(log loss) encode important information\n",
    "- Focusing on such sentences can yield high quality keywords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_body =sample_email()\n",
    "print(email_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk sent_tokenize\n",
    "# import pretty print\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# split the email body into sentences\n",
    "sentences = sent_tokenize(email_body)\n",
    "\n",
    "# Remove sentences that have less than 5 words\n",
    "sentences = [sentence for sentence in sentences if len(sentence.split()) > 5]\n",
    "pprint(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highest_entropy_sentence(sentences, model, tokenizer):\n",
    "# For each sentence compute the average entropy of the tokens\n",
    "    sentences_entropy = []\n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence\n",
    "        tokenized_sentence = tokenizer(tokenizer.bos_token + sentence, return_tensors=\"pt\", add_special_tokens = True)\n",
    "        # Get the entropy of the tokens\n",
    "        sentence_tokens_entropy, relevant, _ = get_tokens_entropy(model, tokenized_sentence)\n",
    "        # Get the average entropy of the tokens\n",
    "        sentence_entropy = sentence_tokens_entropy.mean()\n",
    "        sentences_entropy.append(sentence_entropy)\n",
    "\n",
    "    # Get the sentence with the highest entropy\n",
    "    most_uncertain_sentence = sentences[sentences_entropy.index(max(sentences_entropy))]\n",
    "    return most_uncertain_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_uncertain_sentence = get_highest_entropy_sentence(sentences, model, tokenizer)\n",
    "pprint(most_uncertain_sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "should_display_entropy = True\n",
    "\n",
    "tokenized_most_uncertain_sentence = tokenizer(tokenizer.bos_token + most_uncertain_sentence, return_tensors=\"pt\", add_special_tokens = True)\n",
    "\n",
    "\n",
    "entropy , relevant_tokens, logits = get_tokens_entropy(model, tokenized_most_uncertain_sentence)\n",
    "\n",
    "decode_and_print_tokens_with_entropy(relevant_tokens, entropy, should_display_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the n keys with the highest entropy\n",
    "top_n_tokens = get_top_n_entropy_tokens(entropy, relevant_tokens, 5)\n",
    "top_n_tokens_decoded = tokenizer.batch_decode(top_n_tokens)\n",
    "\n",
    "print(top_n_tokens_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ngram length\n",
    "n = 2\n",
    "# number of pairs to print \n",
    "k = 3\n",
    "decoded = tokenizer.batch_decode(relevant_tokens)\n",
    "print(\"Pairs of tokens with the highest entropy:\")\n",
    "get_k_pairs_of_highest_entropy(entropy, decoded, k, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use keybert to extract keywords from the most uncertain sentence\n",
    "keywords = kw_model.extract_keywords(most_uncertain_sentence, keyphrase_ngram_range=(1, 2), stop_words=None)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "- There is some matching between keybert and the outputs with the highest entropy of GPT2\n",
    "- However we can do better, for the task of keywords extraction bi-directional context could improve results.\n",
    "- Removal of stop-words, punctuation can improve results of key-phrases(but we need to remove them after the calculation of entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Bi-Directional Encoder for Keyword Prediction \n",
    "\n",
    "### Motivation:\n",
    "- Bi-directional encoder has context from both sides which will surely help in terms of keyword identification task.\n",
    "\n",
    "### Action plan:\n",
    "- We'll use a bi-directional encoder model to MASK each token in turn and get the highest cross entropy tokens.\n",
    "- We'll filter out stopwords, determiners, conjunctions and pronouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roberta \n",
    "- Base model is similar size to Bert-Large and GPT2 (335mil params)\n",
    "- Same tokenizer as GPT2(different than bert which uses WordPiece)\n",
    "- Different training scheme than bert(removing next sentence prediction, dynamic masking scheme, bigger batches, longer sequences, longer train time)\n",
    "- https://huggingface.co/roberta-base\n",
    "- Paper: https://arxiv.org/pdf/1907.11692.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token = \"<mask>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_from_text(text):\n",
    "    doc = nlp(text)\n",
    "    words = [token.text for token in doc]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_mask_text_sequences(words):\n",
    "    mask_sequences = []\n",
    "    for i in range(len(words)):\n",
    "        mask_sequence = words.copy()\n",
    "        mask_sequence[i] = mask_token\n",
    "        # Concat the words list into a string\n",
    "        mask_sequence = \" \".join(mask_sequence)\n",
    "        mask_sequences.append({\"mask_sequence\": mask_sequence, \"target\": words[i]})\n",
    "    return mask_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline('fill-mask', model='roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import roberta-base fast tokenizer\n",
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer_roberta = RobertaTokenizerFast.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_mail_in_subwords(tokenizer, email_body):\n",
    "    # Tokenize the email body\n",
    "    tokenized_email = tokenizer(email_body,return_tensors=\"pt\", truncation=True, add_special_tokens=False)\n",
    "    # Get the subwords\n",
    "    subwords = tokenizer.batch_decode(tokenized_email[\"input_ids\"][0])\n",
    "    # Print the subwords\n",
    "    return subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scored_strings(email_body, use_get_words_from_text = False, should_remove_non_verbs_and_nouns = False):\n",
    "    words = get_words_from_text(email_body) if use_get_words_from_text else output_mail_in_subwords(tokenizer_roberta, email_body)\n",
    "    mask_sequences = generate_mask_text_sequences(words)\n",
    "    token_strings = []\n",
    "    entropies = []\n",
    "    target_strings = []\n",
    "\n",
    "    # For each mask sequence, call pipeline with the mask sequence and target word\n",
    "    for mask_sequence in mask_sequences:\n",
    "        target = mask_sequence[\"target\"]\n",
    "        res= unmasker(mask_sequence[\"mask_sequence\"], targets=mask_sequence[\"target\"])[0]\n",
    "        # log2 the score\n",
    "        entropy = -np.log2(res[\"score\"])\n",
    "        token_str = res[\"token_str\"]\n",
    "        token_strings.append(token_str)\n",
    "        entropies.append(entropy)\n",
    "        target_strings.append(mask_sequence[\"target\"])\n",
    "    \n",
    "    return token_strings, entropies, target_strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random email\n",
    "email_body = sample_email()\n",
    "print(email_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the analysis on token level or word level\n",
    "use_get_words_from_text = False\n",
    "token_strings, entropies, target_strings = get_scored_strings(email_body, use_get_words_from_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokens_with_entropy(token_strings, entropies):\n",
    "    for i in range(len(token_strings)):\n",
    "        print(f\"({token_strings[i]}, {entropies[i]}, {i})\", end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "# Import punctuation\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "determiners = [\"the\", \"a\", \"an\", \"this\", \"that\", \"these\", \"those\"]\n",
    "conjunctions = [\"and\", \"or\", \"but\", \"because\", \"so\", \"yet\", \"for\"]\n",
    "pronouns = [\"he\", \"she\", \"it\", \"they\", \"we\", \"you\", \"I\", \"me\", \"him\", \"her\", \"them\", \"us\", \"you\"]\n",
    "# Create a set out of the following lists\n",
    "stop_words = set(STOP_WORDS)\n",
    "stop_words.update(determiners)\n",
    "stop_words.update(conjunctions)\n",
    "stop_words.update(pronouns)\n",
    "stop_words.update(punctuation)\n",
    "\n",
    "def remove_stop_words(token_strings, entropies):\n",
    "    indices_to_remove = []\n",
    "    token_strings = token_strings.copy()\n",
    "    # remove spaces from token strings\n",
    "    token_strings = [token_string.replace(\" \", \"\") for token_string in token_strings]\n",
    "    \n",
    "    # check which indices to remove\n",
    "    for i in range(len(token_strings)):\n",
    "        if token_strings[i] in stop_words:\n",
    "            indices_to_remove.append(i)\n",
    "    \n",
    "    filtered_tokens_strings, filtered_entropies = [], []\n",
    "\n",
    "    for i in range(len(token_strings)):\n",
    "        if i not in indices_to_remove:\n",
    "            filtered_tokens_strings.append(token_strings[i])\n",
    "            filtered_entropies.append(entropies[i])\n",
    "\n",
    "    return filtered_tokens_strings, filtered_entropies\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_k_pairs_of_highest_entropy_roberta(token_strings, entropies, number_of_keywords, n_gram_length, should_remove_stop_words = True):\n",
    "\n",
    "    entropies, token_strings = entropies.copy(), token_strings.copy()\n",
    "    if should_remove_stop_words:\n",
    "        token_strings, entropies = remove_stop_words(token_strings, entropies)\n",
    "    \n",
    "    entropy_pairs = []\n",
    "    entropy_pairs_tokens = []\n",
    "    temp = len(entropies) - n_gram_length\n",
    "    # Fill tensor with average entropy of each k pair\n",
    "    for i in range(len(entropies) - n_gram_length + 1):\n",
    "        entropy_pairs.append(np.mean(entropies[i:i+n_gram_length]))\n",
    "        entropy_pairs_tokens.append(token_strings[i:i+n_gram_length])\n",
    "    \n",
    "    if len(entropy_pairs) != len(entropy_pairs_tokens):\n",
    "        print(\"Here\")\n",
    "\n",
    "    # Choose the n pairs with the highest average entropy\n",
    "    top_n_pairs = get_top_k_indices(entropy_pairs, number_of_keywords)\n",
    "\n",
    "    # Get original \n",
    "    top_n_pairs_tokens = [entropy_pairs_tokens[i] for i in top_n_pairs]\n",
    "\n",
    "    print(top_n_pairs_tokens)\n",
    "\n",
    "    return top_n_pairs_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print_tokens_with_entropy(target_strings if use_get_words_from_text else token_strings, entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(email_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With stop words filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "k = 4\n",
    "\n",
    "top_n_pairs_tokens = print_k_pairs_of_highest_entropy_roberta(target_strings if use_get_words_from_text else token_strings, entropies, k, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without stop words filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_pairs_tokens = print_k_pairs_of_highest_entropy_roberta(target_strings if use_get_words_from_text else token_strings, entropies, k, n, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison to GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GP2 to extract keywords from the email body\n",
    "# Tokenize the email body\n",
    "tokenized_email_body = tokenizer(tokenizer.bos_token + email_body, return_tensors=\"pt\", add_special_tokens = True)\n",
    "\n",
    "# Get the entropy of the tokens\n",
    "email_body_tokens_entropy, relevant, _ = get_tokens_entropy(model, tokenized_email_body)\n",
    "\n",
    "decoded = tokenizer.batch_decode(relevant)\n",
    "print(\"Pairs of tokens with the highest entropy:\")\n",
    "get_k_pairs_of_highest_entropy(email_body_tokens_entropy, decoded, k, n)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison to Keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use keybert to extract keywords from the email body\n",
    "keywords = kw_model.extract_keywords(email_body, keyphrase_ngram_range=(1, n))\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "- Entropy isn't always a good indicator of saliency as humans conceive it- high entropy might be in names or specific phrases\n",
    "- Sentence embedding with cosine similarity on keywords might be a better option for some cases(embedding space is noun centric)\n",
    "- A bi-directional model works better for the task, since it has context from both sides.\n",
    "- Entropy comparison can be used for other purposes - such as evaluation of a model memorizing training data(Collin Raffel example) - https://arxiv.org/pdf/2012.07805.pdf\n",
    "This paper showcases that LLMs are prone to memoizing data even if it appeared once during a training loop.\n",
    "Which is important identification of PII data being exposed by the model. By comparing the entropy with an unbiased source such as zlib or other LMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges:\n",
    "\n",
    "- Tried to fine-tune the models on emails - can't due to compute (could have used distilled gpt but I think it would have hurt the perplexity more)\n",
    "- Conditional entropy is calculated on tokenized words which can be subwords - which makes it harder to get actual words back.\n",
    "- To calculate the entropy/conditional you can't mutate the sentence before the calculation, only after it's calculated you can identify stopwords/nouns/verbs from tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work:\n",
    "\n",
    "- Do better filtering after tokenization(leave only verbs and nouns)\n",
    "- Evaluate method on existing benchmarks(semeval 2010 task 5, semeval 2017 task 10) - https://aclanthology.org/S10-1004/, https://paperswithcode.com/dataset/semeval2017"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "64e08e2444c796b702b473c4ffb135831e2a608e80569b068135e05fb196445e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
