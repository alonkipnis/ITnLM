{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c049e1b5-dec7-47ed-8478-f5e1c808da81",
   "metadata": {},
   "source": [
    "To install dependencies run the following commands:\n",
    "```bash\n",
    "conda create -n env python=3.10\n",
    "conda activate env \n",
    "pip install transformers[torch] pandas jupyterlab scipy google-generativeai\n",
    "jupyter lab\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e03786",
   "metadata": {},
   "source": [
    "## Text Generation and Prompting\n",
    "\n",
    "Moshe Barboy\n",
    "\n",
    "Information Theory and Language Models, Spring 2024\n",
    "\n",
    "School of Computer Science\n",
    "\n",
    "Reichman University\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b992538f-978c-4194-b9f5-7a2378aaf088",
   "metadata": {},
   "source": [
    "# Chain of thought prompring "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042e3e86-bd66-4e85-8396-bcc7925da4ed",
   "metadata": {},
   "source": [
    "![chainofthough](chainofthought.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15c60ec-7011-4a13-8813-26a1c4b69fab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## abstract\n",
    "\n",
    "In this presentation we will see how to generate text with language models and how reformulating the models input can make it better (given the model is big enough or were trained in some specific way).I hope that in the end of the presentation everything related to the Chain-of-Though article will be cleared. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7102fdf5-ec4d-40cb-91c3-6af338b96dc0",
   "metadata": {},
   "source": [
    "## Agenda \n",
    "- Main sources\n",
    "- Language model architecture (specifically GPT1/2/3)\n",
    "- Tokenization and context - you will be amazed from the amount of memmory todays models have.\n",
    "- Generation tactics (temperature, beam search, repetition penalty) - AKA don't ever trust other peoples code\n",
    "- Prompting taxonomy and X-Shot learning, $X \\in ${\"Zero\",\"One\",\"Few\"}\n",
    "- Chain of Though (CoT) prompting - Thats where I review the relevant article and the picture above\n",
    "- Experiment - last letter concatenation (using PaLM 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c09e068-2be6-4745-9c51-2b876882a789",
   "metadata": {},
   "source": [
    "## Main sources \n",
    "\n",
    "\n",
    "\n",
    "[https://www.promptingguide.ai/introduction](https://www.promptingguide.ai/introduction) \n",
    "\n",
    "The blog reference used to construct this presentation. It contains a general overview and some deeper explanations of prompt engineering.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b6d371-a7db-475c-9edd-2425c4f8b3c8",
   "metadata": {},
   "source": [
    "[Language Models are Few-Shot Learners\n",
    "](https://arxiv.org/pdf/2005.14165) <- GPT3 article. Here prompt engineering was introduced for the GPT3 model. The article shows how big a model needs to be for pormt engineering to work in the first place. \n",
    "\n",
    "[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\n",
    "](https://arxiv.org/pdf/2201.11903) <- The article this all presentation is basically about\n",
    "\n",
    "[Automatic Chain of Thought Prompting in Large Language Models\n",
    "](https://arxiv.org/pdf/2210.03493) <- some more advance stuff I didn't get to dive into. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9717fa81-acd5-44b7-a699-28cdd7980c63",
   "metadata": {},
   "source": [
    "## LM architecture \n",
    "\n",
    "A very general overview of the models we will talk about "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750e836e-7c88-46c6-9b87-22e158add813",
   "metadata": {},
   "source": [
    "### [Attention is all you need](https://arxiv.org/pdf/1706.03762)\n",
    "GPT was based on the transformer architecture. Only the decoder part without the cross attention mechanizm was used for GPT. Later the model had improvements but the general architecture remained similar. \n",
    "![transformer](transformer.png)\n",
    "\n",
    "### [GPT](https://gwern.net/doc/www/s3-us-west-2.amazonaws.com/d73fdc5ffa8627bce44dcda2fc012da638ffb158.pdf) \n",
    "\n",
    "A variant of the transformers architecture (they take only the decoder): \n",
    "\n",
    "![GPT](gpt.png)\n",
    "\n",
    "\n",
    "\n",
    "### [GPT2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "Same as GPT + Layer normalization was moved from the output to the input of each sub-block and an additional layer normalization was added after the final self-attention block\n",
    "\n",
    "### [GPT3](https://arxiv.org/pdf/2005.14165)\n",
    "\n",
    "same as GPT2 with \"alternating dense and locally banded sparse attention patterns in the layers of the transformer\"???\n",
    "\n",
    "[sparse transformers](https://openai.com/index/sparse-transformer/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c332e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (23.3.2)\n",
      "Collecting pip\n",
      "  Downloading pip-24.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-24.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.3.2\n",
      "    Uninstalling pip-23.3.2:\n",
      "      Successfully uninstalled pip-23.3.2\n",
      "Successfully installed pip-24.1\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb9b937e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "323fef8b-1a3f-44af-bc1f-2a1c698693bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the imports that were used in this notebook\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import scipy\n",
    "import zlib\n",
    "import json \n",
    "import tqdm\n",
    "import pprint\n",
    "#import google.generativeai as palm\n",
    "import itertools \n",
    "import pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859b16c9-833a-4ffb-8f4b-5fd41a3877a1",
   "metadata": {},
   "source": [
    "As a first try to deal with prompt engineering I load the GPT2 model. It is not a good model for this task as it is small but I find it easier to develop using it due to speed and resource requirement. \n",
    "\n",
    "Together with the model we load the tokenizer of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "224c3819-c79d-49ce-b33a-c21a2c34bd89",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForCausalLM requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mAutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/utils/import_utils.py:1475\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1475\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/utils/import_utils.py:1463\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1461\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1463\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nAutoModelForCausalLM requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf230a2-199b-4014-a537-ccd4b7ef7367",
   "metadata": {},
   "source": [
    "### GPT2 architecture - code view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fcd954-27dc-49c9-8e3d-b104d1fd8058",
   "metadata": {},
   "source": [
    "Here we can see how the architecture of the GPT2 is implemented in the Hugging Face transformers package with Pytorch. \n",
    "printing the one of the blocks in the transformer shows the components inside, to see how they are connected one needs to also print the content of the models \"forward\" method (specific to implementations of models in Pytorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed670c74-6ae3-4a77-9f6f-ea09338eea59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Block(\n",
       "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (attn): GPT2Attention(\n",
       "    (c_attn): Conv1D()\n",
       "    (c_proj): Conv1D()\n",
       "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp): GPT2MLP(\n",
       "    (c_fc): Conv1D()\n",
       "    (c_proj): Conv1D()\n",
       "    (act): NewGELUActivation()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa6bd91-57c7-4f62-b130-e6a77c4f625c",
   "metadata": {},
   "source": [
    "printed using model.transformer.h[0].forward??\n",
    "```python\n",
    "residual = hidden_states\n",
    "hidden_states = self.ln_1(hidden_states)\n",
    "attn_outputs = self.attn(\n",
    "    hidden_states,\n",
    "    layer_past=layer_past,\n",
    "    attention_mask=attention_mask,\n",
    "    head_mask=head_mask,\n",
    "    use_cache=use_cache,\n",
    "    output_attentions=output_attentions,\n",
    ")\n",
    "attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n",
    "outputs = attn_outputs[1:]\n",
    "# residual connection\n",
    "hidden_states = attn_output + residual\n",
    "\n",
    "residual = hidden_states\n",
    "hidden_states = self.ln_2(hidden_states)\n",
    "feed_forward_hidden_states = self.mlp(hidden_states)\n",
    "# residual connection\n",
    "hidden_states = residual + feed_forward_hidden_states\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c05b97-5899-4bb0-a711-553296ab6998",
   "metadata": {},
   "source": [
    "## Tokenizer (BPE Tokenization) and context \n",
    "This part I want to give some intuition about how much memory large language models (LLM) have. We will check out the tokenizer of GPT2 and the size of its context. Then we will compare the memory of different OpenAI LLMs.  \n",
    "\n",
    "This section is not about diving into the tokenizer itself. You can do that [here](https://arxiv.org/pdf/1508.07909) if you want. Generally the token dictionary is created by starting from all the characters available and adding more complicated character combinations based on the most common pairs in som e training corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578d9f7b-08cb-450a-9624-4bba5b9d20c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(64, 'a'), (275, ' b'), (65, 'b'), (269, ' c')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(t,tokenizer.decode(t)) for t in tokenizer.encode(\"a bb c\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce6eb6-d97c-4ba3-a2ff-4137b167ccb3",
   "metadata": {},
   "source": [
    "An important thing to notice is that spaces are incorporated in the tokens and some strings have a separate token for appearing with or without a space. \n",
    "\n",
    "Here are the example for the first and last tokens in the GPT2 tokenizer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519c9355-c29d-4467-a748-d7ef822e0da0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '!'),\n",
       " (1, '\"'),\n",
       " (2, '#'),\n",
       " (3, '$'),\n",
       " (4, '%'),\n",
       " (5, '&'),\n",
       " (6, \"'\"),\n",
       " (7, '('),\n",
       " (8, ')'),\n",
       " (9, '*'),\n",
       " (10, '+'),\n",
       " (11, ','),\n",
       " (12, '-'),\n",
       " (13, '.'),\n",
       " (14, '/'),\n",
       " (15, '0'),\n",
       " (16, '1'),\n",
       " (17, '2'),\n",
       " (18, '3'),\n",
       " (19, '4'),\n",
       " (20, '5'),\n",
       " (21, '6'),\n",
       " (22, '7'),\n",
       " (23, '8'),\n",
       " (24, '9'),\n",
       " (25, ':'),\n",
       " (26, ';'),\n",
       " (27, '<'),\n",
       " (28, '='),\n",
       " (29, '>'),\n",
       " (30, '?'),\n",
       " (31, '@'),\n",
       " (32, 'A'),\n",
       " (33, 'B'),\n",
       " (34, 'C'),\n",
       " (35, 'D'),\n",
       " (36, 'E'),\n",
       " (37, 'F'),\n",
       " (38, 'G'),\n",
       " (39, 'H'),\n",
       " (40, 'I'),\n",
       " (41, 'J'),\n",
       " (42, 'K'),\n",
       " (43, 'L'),\n",
       " (44, 'M'),\n",
       " (45, 'N'),\n",
       " (46, 'O'),\n",
       " (47, 'P'),\n",
       " (48, 'Q'),\n",
       " (49, 'R'),\n",
       " (50, 'S'),\n",
       " (51, 'T'),\n",
       " (52, 'U'),\n",
       " (53, 'V'),\n",
       " (54, 'W'),\n",
       " (55, 'X'),\n",
       " (56, 'Y'),\n",
       " (57, 'Z'),\n",
       " (58, '['),\n",
       " (59, '\\\\'),\n",
       " (60, ']'),\n",
       " (61, '^'),\n",
       " (62, '_'),\n",
       " (63, '`'),\n",
       " (64, 'a'),\n",
       " (65, 'b'),\n",
       " (66, 'c'),\n",
       " (67, 'd'),\n",
       " (68, 'e'),\n",
       " (69, 'f'),\n",
       " (70, 'g'),\n",
       " (71, 'h'),\n",
       " (72, 'i'),\n",
       " (73, 'j'),\n",
       " (74, 'k'),\n",
       " (75, 'l'),\n",
       " (76, 'm'),\n",
       " (77, 'n'),\n",
       " (78, 'o'),\n",
       " (79, 'p'),\n",
       " (80, 'q'),\n",
       " (81, 'r'),\n",
       " (82, 's'),\n",
       " (83, 't'),\n",
       " (84, 'u'),\n",
       " (85, 'v'),\n",
       " (86, 'w'),\n",
       " (87, 'x'),\n",
       " (88, 'y'),\n",
       " (89, 'z'),\n",
       " (90, '{'),\n",
       " (91, '|'),\n",
       " (92, '}'),\n",
       " (93, '~'),\n",
       " (94, '�'),\n",
       " (95, '�'),\n",
       " (96, '�'),\n",
       " (97, '�'),\n",
       " (98, '�'),\n",
       " (99, '�')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(t,tokenizer.decode(t)) for t in range(0,100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302a13a9-df82-4d9d-b3c3-9eaaf605a35d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50250, ' amplification'),\n",
       " (50251, 'ominated'),\n",
       " (50252, ' regress'),\n",
       " (50253, ' Collider'),\n",
       " (50254, ' informants'),\n",
       " (50255, ' gazed'),\n",
       " (50256, '<|endoftext|>'),\n",
       " (50257, ''),\n",
       " (50258, ''),\n",
       " (50259, '')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(t,tokenizer.decode(t)) for t in range(50250,50260)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c1d06-bf2c-42bc-8d65-32184bb088a4",
   "metadata": {},
   "source": [
    "Lets check how many tokens we need to describe some relatively long looking text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e0cfc-50c9-4eb1-8fbb-1b8c55cda060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"\"\"\n",
    "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "A: The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "A: The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "A: The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A: \n",
    "\n",
    "The answer is True.\n",
    "\"\"\"\n",
    "\n",
    "len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51c56cc-5295-4016-a308-00fab3926593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\n",
    "    tokenizer.encode(text)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b93973e-6c63-498e-8fab-41699b6a49a7",
   "metadata": {},
   "source": [
    "The above example showed that we need 192 tokens for 120 words. Given that the model have a context of 1024 tokens, the model memorization ability is very impressive even for GPT2 which is already considered encient. Here is a table for how much the GPT models grew bigger in the last years and how much their context increased. Consider that an older LSTM versions were generally good at memorizing about 30 elements back (before attention mechanism was introduced), and for n-grams the maximal context is n which is usually even smaller than that.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c4d653-e6e3-4a9c-a5ac-36283cdcdd5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ctx</th>\n",
       "      <th>parameters($10^9$)</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GPT</th>\n",
       "      <td>512</td>\n",
       "      <td>0.117</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT-2</th>\n",
       "      <td>1024</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT-3</th>\n",
       "      <td>2048</td>\n",
       "      <td>175</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT-4</th>\n",
       "      <td>16000</td>\n",
       "      <td>~1760</td>\n",
       "      <td>2021?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT-4o</th>\n",
       "      <td>128000</td>\n",
       "      <td>?</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ctx parameters($10^9$)   year\n",
       "GPT        512              0.117   2018\n",
       "GPT-2     1024                1.5   2019\n",
       "GPT-3     2048                175   2020\n",
       "GPT-4    16000              ~1760  2021?\n",
       "GPT-4o  128000                  ?   2024"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    \"ctx\":[512,1024,2048,16000,128000],\n",
    "    \"parameters($10^9$)\":[0.117,1.5,175,\"~1760\",\"?\"],\n",
    "    \"year\":[2018,2019,2020,\"2021?\",2024],\n",
    "},\n",
    "index = [\"GPT\",\"GPT-2\",\"GPT-3\",\"GPT-4\",\"GPT-4o\"]            \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff724d55-5d69-4122-bf76-8743fe26f2e3",
   "metadata": {},
   "source": [
    "Lets make sure that the GPT2 model is working for a vector of 1024 zeros and crashes for 1025: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023ec192-4f87-4438-b4c2-d71b5f11b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(torch.zeros(1,1024).int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19ab322-98aa-4533-8f5b-697dea2e7014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 50257])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f0e31f-8f51-437c-830e-02e00b562e3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with pytest.raises(IndexError): # this part make the code crash unless an IndexError expection is raised by the code. \n",
    "    with torch.no_grad():\n",
    "        output = model(torch.zeros(1,1025).int())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8400de-aeea-4f55-8a52-6aa60c7eecd7",
   "metadata": {},
   "source": [
    "## Generation tactics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1f9554-b219-4103-8731-a94f5df4936f",
   "metadata": {},
   "source": [
    "In this section we will go over generation tactics. How do we generate text given some promt using the language models? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57882fe4-ddf4-44b3-8737-935531be2599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2437, 460, 530, 1382, 257, 2156, 30]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"How can one build a house?\" # the example prompt we will use throughout this notebook (feel free to change it)\n",
    "tokens = tokenizer.encode(prompt) # converting the prompt to tokens for the model \n",
    "tokens # printing the tokens for out specific prompt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b9e31d-797e-4935-b32c-752a8476687b",
   "metadata": {},
   "source": [
    "The model predicts the next token: given some list of tokens, it returns a score for each token in the vocabulary with the meaning of \"how likely it is the next token\". To generate a string given a pormpt we will generate token by token. On each iteration we will choose the token with the highest score and apped it to the least untill we get the special '<|endoftext|> token or the amount of generated tokens exeeds 50 (threshold given for faster execution). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696e6abb-f59a-458e-bff1-f5138e3e203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = []\n",
    "log_prob = 0\n",
    "for _ in range(50): # generate a new token at most 50 times\n",
    "    # run the model and take the logits of the last token \n",
    "    with torch.no_grad():\n",
    "        tokens_tensor = torch.Tensor(tokens + answer).unsqueeze(0).int()\n",
    "        output = model(tokens_tensor)\n",
    "    logits = output.logits.numpy()[0,-1]    \n",
    "    # choose the token with the highest score and append it to the tokens\n",
    "    next_token = logits.argmax()\n",
    "    if next_token==50256: # 50256='<|endoftext|>'. Stop if stopping token is reached\n",
    "        break\n",
    "    answer.append(next_token)\n",
    "    # update the log probability of the answer\n",
    "    probabilities = scipy.special.softmax(logits)\n",
    "    log_prob += np.log(probabilities[next_token])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ca102d-a54d-45b3-ad71-135c4ed1d8c1",
   "metadata": {},
   "source": [
    "The length of the generated answer is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f9005f-89e6-416d-be3a-f36593b706a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68c7c4e-0a04-4097-ae03-e1d2b16e0307",
   "metadata": {},
   "source": [
    "Lets decode and see how the model completed the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cb865c-5ba1-4014-a189-1a02cf07e43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The answer is simple: build a house.\n",
      "\n",
      "The first step is to build a house.\n",
      "\n",
      "The second step is to build a house.\n",
      "\n",
      "The third step is to build a house.\n",
      "\n",
      "The fourth step is\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91354bc-be95-45e5-85b0-ce51962e5146",
   "metadata": {},
   "source": [
    "## [Temperature](https://arxiv.org/pdf/1503.02531) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a3d1d1-40f2-4fd2-9c3f-d98e6cbd31f5",
   "metadata": {},
   "source": [
    "if we want to make the model random we can apply a softmax on the logits provided by the model and choose tokens based on their probability. With temperature T we can tweek the probabilities to be more uniform. For T=0 we would get the above behavior (the model generates the token with the highest logit), for T=inf we would have a uniform distribution and the model will just randomly generate the tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cb2b0c-5ea9-43bc-93cf-c44e9bd274a1",
   "metadata": {},
   "source": [
    "$$\n",
    "Prob(logit;T)=softmax(logits/T)_j=\\frac{\\exp(logits_j/T)}{\\sum_i\\exp(logit_i/T)} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54a856c-ea39-4e94-b968-6d6420900437",
   "metadata": {},
   "source": [
    "The above code was modified to include logits and temperature. It was also wrapped in a function for easier use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0718294-7d75-4726-a3fa-8a4c77d08a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(prompt,temperature=0,seed=5,max_length=50):\n",
    "    np.random.seed(seed)\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    \n",
    "    answer = []\n",
    "    log_prob = 0\n",
    "    for _ in range(max_length): \n",
    "        with torch.no_grad():\n",
    "            tokens_tensor = torch.Tensor(tokens + answer).unsqueeze(0).int()\n",
    "            output = model(tokens_tensor)\n",
    "        logits = output.logits.numpy()[0,-1]    \n",
    "        if temperature==0: # <- adding the temperature. 0 is the same as argmax. >0 means we sample the next token based on the probabilities\n",
    "            next_token = logits.argmax()\n",
    "        else:\n",
    "            probabilities = scipy.special.softmax(logits/temperature)\n",
    "            next_token = np.random.choice(range(len(probabilities)),size=None,p=probabilities)\n",
    "        if next_token==50256: # 50256='<|endoftext|>'\n",
    "            break\n",
    "        answer.append(next_token)\n",
    "        probabilities = scipy.special.softmax(logits)\n",
    "        log_prob += np.log(probabilities[next_token])\n",
    "    answer_str = tokenizer.decode(answer)\n",
    "    return answer_str, log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8325b330-65b5-4bc9-9a7c-5e67546fe4d9",
   "metadata": {},
   "source": [
    "here are some examples of how we can get a more random output with higher temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2a41c-7fe8-4a82-9ea6-165493e2e70a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -36.7519825608324\n",
      "\n",
      "\n",
      "The answer is simple: build a house.\n",
      "\n",
      "The first step is to build a house.\n",
      "\n",
      "The second step is to build a house.\n",
      "\n",
      "The third step is to build a house.\n",
      "\n",
      "The fourth step is\n"
     ]
    }
   ],
   "source": [
    "temperature=0\n",
    "res,prob=infer(\"How can one build a house?\",temperature=temperature)\n",
    "print(temperature,prob)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3303212-655e-4e70-886d-779b85f5dadf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 -44.870093021811044\n",
      "\n",
      "\n",
      "The answer is simple: build a house.\n",
      "\n",
      "The answer to that question is simple: build a house.\n",
      "\n",
      "Why build a house?\n",
      "\n",
      "The answer to that question is simple: build a house.\n",
      "\n",
      "Why build\n"
     ]
    }
   ],
   "source": [
    "temperature=0.5\n",
    "res,prob=infer(\"How can one build a house?\",temperature=temperature)\n",
    "print(temperature,prob)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ed8f5d-97e2-4dc6-a060-315e2bef36db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -240.43827770277858\n",
      "\n",
      "\n",
      "In Making Up To Nine years ago, the struggling owner of Malva Kuggedelha (the main building in the Kardandelpla area of Mumbai) built. He built 144 units (171 condo) and 311 buildings (106\n"
     ]
    }
   ],
   "source": [
    "temperature=1\n",
    "res,prob=infer(\"How can one build a house?\",temperature=temperature)\n",
    "print(temperature,prob)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b18d59-be3e-462e-b3e8-7c8e0d774f7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 -634.5288157463074\n",
      "adersMorningabsExcellentaliation dens spectral 501statusta property Operator Cannon punish stuffingaha Buddha Strong Founder fiery USSR Mun disagree]; Mes offensive hits flattering unsu trail lastrating setback eaterits RallyYmedium Viktor Sneak Own hr appellate Statenk mechanicoller overhaul idolsrendered\n"
     ]
    }
   ],
   "source": [
    "temperature=10\n",
    "res,prob=infer(\"How can one build a house?\",temperature=temperature)\n",
    "print(temperature,prob)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341173d6-0445-4d6b-be52-b2921bd92439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 -639.5775985717773\n",
      " Points pellets yeah Xander arsenal npmizontal regimes friendshipistance corporradioDamageraction undraftedOME Odd Bear Qin classmates Sang rotation safer Victor facialdiv surgery sucker inciner efficiencyired weigh Weiner030We Wrongoopus 1915 mishand Sheriff HavanaIRO renovated�aundering Kusariusorahestation\n"
     ]
    }
   ],
   "source": [
    "temperature=100\n",
    "res,prob=infer(\"How can one build a house?\",temperature=temperature)\n",
    "print(temperature,prob)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405a8775-a6bc-4646-b75c-432d5e96615e",
   "metadata": {},
   "source": [
    "For temperatures above 1 in our specific case the model just reutrns random tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132d9c87-e6fb-45d9-87c7-03d1966b0eee",
   "metadata": {},
   "source": [
    "### Beam Search  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaf836f-176e-42ab-a0b5-b4f63988afd9",
   "metadata": {},
   "source": [
    "Sometimes the model chooses a wrong combination of words just because they are more common in the language. For example, in the translation of \"I like soccer\" to spanish, the right translation should start with \"Me gusta\". However in Spanish \"Me gustan\" is generally more common which results in the model giving it a higher logits. On later tokens the model realizes the mistake by resulting in a lower overall logits than for the right translation. To enable the model to currect its decision we can use Beam Search, which means we consider 3 possible generation paths at each step and in the end we choose the most likely path overall. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0458a375-906d-4f03-82c5-36fdcc7cddc9",
   "metadata": {},
   "source": [
    "Trasnlating \"I like soccer\" to Spanish example\n",
    "\n",
    "![beam search](beamsearch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e4d8ad-b889-45cf-bf4a-f60a3f181a23",
   "metadata": {},
   "source": [
    "Figure 16-6 from Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow by Aurélien Géron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33470a4f-f965-4106-be40-648d684d8359",
   "metadata": {},
   "source": [
    "The code below is a modified version of the infer method that also supports Beam Search - on each step we consider several tokens instead of just one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f6b638-0028-4d9a-a0ca-0dc76daf9385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(prompt,temperature=0,seed=5,beam_num=1,max_length=50,verbose=False):\n",
    "    \"\"\"\n",
    "    Generates text using GPT2. \n",
    "    Note: On top of the previous method, we are now keeping track of the <beam_num> most probable answers at each step. \n",
    "\n",
    "    input:\n",
    "        prompt - input text \n",
    "        temperature=0 - how much to smooth the probability distribution\n",
    "        seed=5 - seed for the sampling \n",
    "        beam_num=1 - beam size\n",
    "        max_length=50 - when to stop generating \n",
    "        verbose=False - whether to print the answer after each step\n",
    "    \n",
    "    returns: \n",
    "        answer_str - the resulting answer\n",
    "        best_prob - the log probability the model assigned to the resulting answer \n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    answers =[[]]\n",
    "    answer_probs= [0]\n",
    "    for step in range(max_length): \n",
    "        with torch.no_grad():\n",
    "            tokens_tensor = torch.Tensor([tokens + answer for answer in answers]).int()\n",
    "            output = model(tokens_tensor)\n",
    "        logits = output.logits.numpy()[:,-1,:]    \n",
    "        \n",
    "        if temperature==0:\n",
    "            next_tokens = np.argsort(logits)[:,-beam_num:]\n",
    "        else:\n",
    "            probabilities = scipy.special.softmax(logits/temperature,axis=-1)\n",
    "            next_tokens = [np.random.choice(range(len(p_row)),size=beam_num,replace=False,p=p_row) for p_row in probabilities]\n",
    "            next_tokens = np.r_[next_tokens]\n",
    "        next_probs = scipy.special.softmax(logits,axis=-1)\n",
    "\n",
    "        ## generating all new answers and choosing the best <beam_num> answers\n",
    "        new_answers = []\n",
    "        new_answer_probs = []\n",
    "        for answer,answer_prob,next_tokens_row,next_prob in zip(answers,answer_probs,next_tokens,next_probs):\n",
    "            for next_token_inst in next_tokens_row:\n",
    "                new_answers.append(answer+[next_token_inst]) \n",
    "                new_answer_probs.append(answer_prob + np.log(next_prob[next_token_inst]))\n",
    "        best_answer_ids = np.argsort(new_answer_probs)[-beam_num:]\n",
    "        answers = [new_answers[best_answer_id] for best_answer_id in best_answer_ids]\n",
    "        answer_probs = [new_answer_probs[best_answer_id] for best_answer_id in best_answer_ids]\n",
    "\n",
    "        if verbose: # printing result after each step if verbose=True\n",
    "            print(f\"step {step}\")\n",
    "            print(*list(map(tokenizer.decode,answers)),sep=\"\\n\")\n",
    "            print(answer_probs)\n",
    "            print(\"-------------------\")\n",
    "            \n",
    "    # choosing the most probable answer\n",
    "    best_answer = answers[np.argmax(answer_probs)]\n",
    "    best_prob = answer_probs[np.argmax(answer_probs)]\n",
    "    \n",
    "    answer_str = tokenizer.decode(best_answer)\n",
    "    return answer_str, best_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8aa0f8-3f5b-452a-83d8-5841cd662bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Your simple units may spend money (hands-down new terracotta stuff). Long lives depend fully on finishing a complete idea you invented sometime back (written story card is a go though!). Your usual household knows about cheap babysitting polygons and cheap -294.5887782369973\n"
     ]
    }
   ],
   "source": [
    "print(*infer(\"How can one build a house?\",temperature=2,beam_num=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a115f10-051d-49cc-97b9-5a1946746bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " How can one build a house? How can one build a house? How can one build a house? How can one build a house? How can one build a house? How can one build a house? How can one build a house? How -14.82656359457178\n"
     ]
    }
   ],
   "source": [
    "print(*infer(\"How can one build a house?\",temperature=0,beam_num=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47880de-037e-49b4-b83f-96b4b3375373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In Making Up To Nine years ago, the struggling owner of Malva Kuggedelha (the main building in the Kardandelpla area of Mumbai) built. He built 144 units (171 condo) and 311 buildings (106 -240.43827770277858\n"
     ]
    }
   ],
   "source": [
    "print(*infer(\"How can one build a house?\",temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c3345-56d8-49cc-9913-3e359d3fb02a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The answer is simple: build a house.\n",
      "\n",
      "The first step is to build a house.\n",
      "\n",
      "The second step is to build a house.\n",
      "\n",
      "The third step is to build a house.\n",
      "\n",
      "The fourth step is -36.7519825608324\n"
     ]
    }
   ],
   "source": [
    "print(*infer(\"How can one build a house?\",temperature=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92efe34-a18f-48f9-98ac-4700f465ec20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0\n",
      " shredded\n",
      " Ter\n",
      "[-21.747697830200195, -9.572603225708008]\n",
      "-------------------\n",
      "step 1\n",
      " Ter farmers\n",
      " TerWAR\n",
      "[-25.038633346557617, -24.506505012512207]\n",
      "-------------------\n",
      "step 2\n",
      " Ter farmers hoping\n",
      " TerWAR issues\n",
      "[-35.27052307128906, -34.76681995391846]\n",
      "-------------------\n",
      "step 3\n",
      " TerWAR issues win\n",
      " Ter farmers hoping for\n",
      "[-46.107449531555176, -37.76639771461487]\n",
      "-------------------\n",
      "step 4\n",
      " Ter farmers hoping for downt\n",
      " Ter farmers hoping for freedom\n",
      "[-53.32565426826477, -45.25116801261902]\n",
      "-------------------\n",
      " Ter farmers hoping for freedom -45.25116801261902\n"
     ]
    }
   ],
   "source": [
    "print(*infer(\"How can one build a house?\",temperature=2,beam_num=2,verbose=True,max_length=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbd83db-5a29-402d-a86c-de5756488c77",
   "metadata": {},
   "source": [
    "## A way to deal with repetition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646350f1-8283-4ff0-a153-a385d7a64c2d",
   "metadata": {},
   "source": [
    "An interesting way to deal with repetition in generation is by compressing the resulting answer. If the compression ratio is too high, it means there are probably repetitions in the answer and we can try to rise the temperature and generate an answer again. Here is an example of such tactic: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11579914-49fa-4846-baff-0a5ed9f54b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The Hindu Lord says that by working in a manner that is conducive to his own welfare, his life can be brought to a liking by the one who works hard, for, as he indicates, who wants to be good stuff, there is\n",
      "1.4217687074829932 -156.10825242474675 0.8\n"
     ]
    }
   ],
   "source": [
    "def compression_ratio(text):\n",
    "    text_bytes = text.encode(\"utf-8\")\n",
    "    return len(text_bytes) / len(zlib.compress(text_bytes))\n",
    "\n",
    "compression_threshold = 1.5 \n",
    "\n",
    "for temperature in [0,0.2,0.5,0.8,1,3,5,10]:\n",
    "    answer, prob = infer(\"How can one build a house?\",temperature=temperature)\n",
    "    if compression_ratio(answer)<compression_threshold:\n",
    "        break\n",
    "print(answer)\n",
    "print(compression_ratio(answer),prob,temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c99dc5f-f1b3-4c8b-9d25-b2d11e6458fb",
   "metadata": {},
   "source": [
    "## Never implement yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f57031-c6db-4861-945c-75125f2b43df",
   "metadata": {},
   "source": [
    "Turns out that Hugging Face already implemented multiple generation tactics into the libarary. This comes to show that implementing this on your own is not a must. \n",
    "\n",
    "Here is the implementation of all of the above and much much more in Hugging Face "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff1448e-61da-497d-9098-04c57332d311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' What the heck does it mean?\\n\\nIf you want a house, there are several steps you can take.\\n\\nBuild Your House\\n\\nYou want to build your home by finding the right materials to make the home. These will give you some idea of the level of care and care required to maintain a home. These may include materials such as plywood and concrete, but this is usually also where you look for the \"right materials\".\\n\\nBuilding Materials\\n\\nIf your home is'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def infer_hf(input_text,**kwargs):\n",
    "    input_ids = tokenizer.encode(input_text,return_tensors=\"pt\")\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        generation_config=transformers.generation.configuration_utils.GenerationConfig(\n",
    "            **kwargs\n",
    "        )\n",
    "    )\n",
    "    output_text = tokenizer.decode(output_ids.numpy()[0])\n",
    "    answer = output_text[len(input_text):]\n",
    "    return answer\n",
    "\n",
    "\n",
    "infer_hf(\n",
    "    \"How one builds a house?\",\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    num_beams=3,\n",
    "    temperature=2.,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1 # not sure if that is exactly equivalent to what I did. \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306f3c47-1ccc-4e8b-9815-49e78b5f93c8",
   "metadata": {},
   "source": [
    "# Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b8100d-ffdd-415a-acbd-2a245efc01b8",
   "metadata": {},
   "source": [
    "Now that we know how the LLM works and how it can be used to give an answer to some prompt, lets talk about the prompt itself and how it can be constructed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cf25ff-ad7a-4f1a-9048-b29d0bd5ceac",
   "metadata": {},
   "source": [
    "## examples of prompts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a1a181-c032-40e4-9609-a36136d81d9e",
   "metadata": {},
   "source": [
    "Prompt:\n",
    "```\n",
    "### Instruction ###\n",
    "Translate the text below to Spanish:\n",
    "\n",
    "Text: \"hello!\"\n",
    "```\n",
    "Output:\n",
    "```\n",
    "¡Hola!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2256ad8-1d81-4c68-b465-b1c583d5b77d",
   "metadata": {},
   "source": [
    "Prompt:\n",
    "```\n",
    "Q: <Question>?\n",
    "A: <Answer>\n",
    "\n",
    "Q: <Question>?\n",
    "A: <Answer>\n",
    "\n",
    "Q: <Question>?\n",
    "A: <Answer>\n",
    "\n",
    "Q: <Question>?\n",
    "A:\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266a1ea6-a843-4ae3-ac41-30e3e088e09e",
   "metadata": {},
   "source": [
    "## Prompt elements \n",
    "\n",
    "\n",
    "instruction, context, input data, output indicator:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f358f467-4d20-471d-b2ab-402d742a0b15",
   "metadata": {},
   "source": [
    "Instruction - a specific task or instruction you want the model to perform\n",
    "\n",
    "Context - external information or additional context that can steer the model to better responses\n",
    "\n",
    "Input Data - the input or question that we are interested to find a response for\n",
    "\n",
    "Output Indicator - the type or format of the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74256d1b-0e9f-45d5-928f-557e68036750",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "Prompt:\n",
    "```\n",
    "Classify the text into neutral, negative, or positive <- instruction\n",
    "Text: I think the food was okay. <- data\n",
    "Sentiment:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6addfccb-edd7-4def-a8c3-16d7c914f12d",
   "metadata": {},
   "source": [
    "## zero/one/few shots "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfc185f-18de-476c-bfe3-a200179dd205",
   "metadata": {},
   "source": [
    "The GPT3 article introduced the idea of \"training\" the model with new example directly using the prompt. The intuition behind it is that by giving a task discription and some examples to a task, you can help the model fetch the relevant knowledge it seen during the training. They distinguish between different Xshot tasks, where X is the amount of example you showed the model. It is explained in the following figure in the article: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903aa1d1-c358-46ef-84ba-c90e96dd0cc1",
   "metadata": {},
   "source": [
    "![prompts](promptshots.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebc50b7-b798-4440-acfa-fb6826227e3c",
   "metadata": {},
   "source": [
    "Example of a few shot learning: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ca282-85e8-4626-a886-0215d8fa589d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\nThe answer is True.\\n\\nThe odd', -1.5613370465289336)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(\"\"\"\n",
    "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "A: The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "A: The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "A: The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A:\n",
    "\"\"\",max_length=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6633e8-4499-49dc-a4a9-f4d5eabd3511",
   "metadata": {},
   "source": [
    "Another point the article made was, that in order for the prompting to work, one must use a large enough model. They trained models with different parameter amounts and evaluated them on some simple linguistic tasks like word unscrambling. The results are presented in the next figure: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11607ed3-d206-4df8-aec4-8207ed9c9a6a",
   "metadata": {},
   "source": [
    "![bigmodelsneeded](bigmodelsneeded.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61af1d2-99d8-47fd-a781-823e6f33b263",
   "metadata": {},
   "source": [
    "From this figure it can be deduced that a GPT2 model that only has around 1B parameters is not a good choice for prompting. We will attend this issue later in the presentataion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250ac663-da41-4efc-ae08-63d2b649dad0",
   "metadata": {},
   "source": [
    "# [Chain of thought prompting](https://arxiv.org/pdf/2201.11903) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588328ba-bf4d-4d2c-b6fe-49357527b836",
   "metadata": {},
   "source": [
    "We got to the main topic of this presentation. Now that you understand how big the scope of modern LLMs, how to use them for generation, and what prompting actually is, we can talk about nuances in the prompting itself.  \n",
    "The paper shows that for middle school level math problems, using few-shot learning with fully explained answers improves the models accuracy:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276fe0b8-1e14-4e6a-96d7-0fe9f884bd7b",
   "metadata": {},
   "source": [
    "![chainofthough](chainofthought.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9f8dec-cf63-45dc-ae5d-2fe963e52733",
   "metadata": {},
   "source": [
    "We can try to measure the effect with GPT2 though the results are not expected to be good: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58342052-821c-4147-b27f-128b0b0dfd20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A: The answer is 3.\\nQ: Roger has a lot of money. He has a lot of\\nsugar. He has a lot of money. He has a lot of money.\\nQ: Roger has a lot of money.',\n",
       " -51.93085535179125)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(\"\"\"\n",
    "Q: Roger has 5 tennis balls. He buys 2 more cans of\n",
    "tennis balls. Each can has 3 tennis balls. How many\n",
    "tennis balls does he have now?\n",
    "A: The answer is 11.\n",
    "Q: The cafeteria had 23 apples. If they used 20 to\n",
    "make lunch and bought 6 more, how many apples\n",
    "do they have?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5bdf34-0922-4792-af93-4e09a0077ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A: The cafeteria had 23 apples. If they used 20 tomake lunch and bought 6 more, how many apples do they have now?\\nQ: Roger has a lot of money. He has a lot of money. He has a lot of',\n",
       " -35.67198682320304)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(\"\"\"\n",
    "Q: Roger has 5 tennis balls. He buys 2 more cans of\n",
    "tennis balls. Each can has 3 tennis balls. How many\n",
    "tennis balls does he have now?\n",
    "A: Roger started with 5 balls. 2 cans of 3 tennis balls\n",
    "each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "Q: The cafeteria had 23 apples. If they used 20 to\n",
    "make lunch and bought 6 more, how many apples\n",
    "do they have?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ebeaac-c51e-4343-bcd6-632b2d53604c",
   "metadata": {},
   "source": [
    "### GSM8K dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff27ea71-418f-4e22-95f1-83c6cbb5abf3",
   "metadata": {},
   "source": [
    "One of the datasets used in the article. Here are some examples of the instances in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583331fc-616e-44ca-a15a-55d6f49ec4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gsm8k/train.jsonl\") as f:\n",
    "    gsm=pd.DataFrame([json.loads(l) for l in f])\n",
    "with open(\"gsm8k/test.jsonl\") as f:\n",
    "    gsm_test=pd.DataFrame([json.loads(l) for l in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74600803-8cac-4a1b-b8f8-ddc826ff2660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Natalia sold clips to 48 of her friends in Apr...</td>\n",
       "      <td>Natalia sold 48/2 = &lt;&lt;48/2=24&gt;&gt;24 clips in May...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Weng earns $12 an hour for babysitting. Yester...</td>\n",
       "      <td>Weng earns 12/60 = $&lt;&lt;12/60=0.2&gt;&gt;0.2 per minut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Betty is saving money for a new wallet which c...</td>\n",
       "      <td>In the beginning, Betty has only 100 / 2 = $&lt;&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Julie is reading a 120-page book. Yesterday, s...</td>\n",
       "      <td>Maila read 12 x 2 = &lt;&lt;12*2=24&gt;&gt;24 pages today....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>James writes a 3-page letter to 2 different fr...</td>\n",
       "      <td>He writes each friend 3*2=&lt;&lt;3*2=6&gt;&gt;6 pages a w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7468</th>\n",
       "      <td>Very early this morning, Elise left home in a ...</td>\n",
       "      <td>For the distance she traveled, Elise paid 23 -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7469</th>\n",
       "      <td>Josh is saving up for a box of cookies. To rai...</td>\n",
       "      <td>He makes $.5 profit on each bracelet because 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7470</th>\n",
       "      <td>Colin can skip at six times the speed that Bra...</td>\n",
       "      <td>Tony can skip at twice the speed that Bruce ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7471</th>\n",
       "      <td>Janet, a third grade teacher, is picking up th...</td>\n",
       "      <td>Janet needs 35 lunches for the kids + 5 for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7472</th>\n",
       "      <td>At 30, Anika is 4/3 the age of Maddie. What wo...</td>\n",
       "      <td>If Anika is 30 now, in 15 years, she'll be 30+...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7473 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "0     Natalia sold clips to 48 of her friends in Apr...   \n",
       "1     Weng earns $12 an hour for babysitting. Yester...   \n",
       "2     Betty is saving money for a new wallet which c...   \n",
       "3     Julie is reading a 120-page book. Yesterday, s...   \n",
       "4     James writes a 3-page letter to 2 different fr...   \n",
       "...                                                 ...   \n",
       "7468  Very early this morning, Elise left home in a ...   \n",
       "7469  Josh is saving up for a box of cookies. To rai...   \n",
       "7470  Colin can skip at six times the speed that Bra...   \n",
       "7471  Janet, a third grade teacher, is picking up th...   \n",
       "7472  At 30, Anika is 4/3 the age of Maddie. What wo...   \n",
       "\n",
       "                                                 answer  \n",
       "0     Natalia sold 48/2 = <<48/2=24>>24 clips in May...  \n",
       "1     Weng earns 12/60 = $<<12/60=0.2>>0.2 per minut...  \n",
       "2     In the beginning, Betty has only 100 / 2 = $<<...  \n",
       "3     Maila read 12 x 2 = <<12*2=24>>24 pages today....  \n",
       "4     He writes each friend 3*2=<<3*2=6>>6 pages a w...  \n",
       "...                                                 ...  \n",
       "7468  For the distance she traveled, Elise paid 23 -...  \n",
       "7469  He makes $.5 profit on each bracelet because 1...  \n",
       "7470  Tony can skip at twice the speed that Bruce ca...  \n",
       "7471  Janet needs 35 lunches for the kids + 5 for th...  \n",
       "7472  If Anika is 30 now, in 15 years, she'll be 30+...  \n",
       "\n",
       "[7473 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9b8be7-ef4a-426e-a4ea-f8cd33f17691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mark has a garden with flowers. He planted plants of three different colors in it. Ten of them are yellow, and there are 80% more of those in purple. There are only 25% as many green flowers as there are yellow and purple flowers. How many flowers does Mark have in his garden?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsm.iloc[5].question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b8126b-747f-40e8-8cbc-987c5a058415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There are 80/100 * 10 = <<80/100*10=8>>8 more purple flowers than yellow flowers.\\nSo in Mark's garden, there are 10 + 8 = <<10+8=18>>18 purple flowers.\\nPurple and yellow flowers sum up to 10 + 18 = <<10+18=28>>28 flowers.\\nThat means in Mark's garden there are 25/100 * 28 = <<25/100*28=7>>7 green flowers.\\nSo in total Mark has 28 + 7 = <<28+7=35>>35 plants in his garden.\\n#### 35\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsm.iloc[5].answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b428f0-4e28-47ea-be07-c7670cae2073",
   "metadata": {},
   "source": [
    "The article presents the performance of several LLM models with and without full answers. Giving fully explained answer is defined as \"Chain-of-though prompting\" or COT: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662d4c15-9ce7-4b39-8336-3172c1d850c2",
   "metadata": {},
   "source": [
    "![cotresults](cotresults.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f81ed2-db68-4e94-addd-1b52c42c2bdf",
   "metadata": {},
   "source": [
    "## Experiment - Last letter concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224db83a-b125-4469-bc1c-5e1a5651ca94",
   "metadata": {},
   "source": [
    "To experiment with all of the obove knowledge I tried to recreate the last letter concatenation experiment as described in the article. Taking the most popular names and surenames in the US, combining them into full names and asking the model to return a concatentation of all the last letters in the name. \n",
    "\n",
    "I tried to use GPT2 for this task. It didn't give any coherent results. To make the experiment more interesting, I used PaLM 2 via online API provided by google. This gave more interesting results. However, to run the cells with the API part, you would need to add a google-key that enables you to use it. Also, they might have changed something in their API so the results might not be recreated. \n",
    "\n",
    "To create the API key go to https://pypi.org/project/google-generativeai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa28be-0a10-46c8-8e59-8e1b78ae7fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "surenames = pd.read_csv(\"popular_surenames.csv\").Name.str.lower()\n",
    "firstnames = pd.read_csv(\"popular_firstnames.csv\").Name.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a462f2-2a47-4db5-a172-b7104687c38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       smith\n",
       "1     johnson\n",
       "2    williams\n",
       "3       brown\n",
       "4       jones\n",
       "Name: Name, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surenames.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91ec0ae-49ee-4ec5-b7f3-5b0474fe274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(itertools.product(firstnames,surenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d681bc-689d-4022-92dd-13d078802dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(np.array(names),columns=[\"first\",\"last\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780cb870-8d62-44dc-9810-da4739e7168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.assign(\n",
    "    prompt = dataset[\"first\"]+\" \"+dataset[\"last\"],\n",
    "    reg_sol= \":\" + dataset[\"first\"].str[-1]+dataset[\"last\"].str[-1] + \" \\n\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a88280-0d0e-4eb3-8803-ddb78f8cee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.assign(\n",
    "    cot_sol= \" -> First name ends with \" + dataset[\"first\"].str[-1] + \n",
    "    \", Last name ends with \" + dataset[\"last\"].str[-1] + \",so the solution is \" + dataset[\"reg_sol\"] + \" \\n\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e753c4-e155-42ac-841a-04f1c0a606e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.sample(frac=0.1)\n",
    "test = dataset.iloc[:10]\n",
    "train = dataset.iloc[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e51847-2792-4577-95eb-bbaa7240ec2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>prompt</th>\n",
       "      <th>reg_sol</th>\n",
       "      <th>cot_sol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>311150</th>\n",
       "      <td>ruby</td>\n",
       "      <td>freeman</td>\n",
       "      <td>ruby freeman</td>\n",
       "      <td>:yn \\n</td>\n",
       "      <td>-&gt; First name ends with y, Last name ends wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32142</th>\n",
       "      <td>logan</td>\n",
       "      <td>mcdonald</td>\n",
       "      <td>logan mcdonald</td>\n",
       "      <td>:nd \\n</td>\n",
       "      <td>-&gt; First name ends with n, Last name ends wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120849</th>\n",
       "      <td>enzo</td>\n",
       "      <td>benjamin</td>\n",
       "      <td>enzo benjamin</td>\n",
       "      <td>:on \\n</td>\n",
       "      <td>-&gt; First name ends with o, Last name ends wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471582</th>\n",
       "      <td>londyn</td>\n",
       "      <td>mora</td>\n",
       "      <td>londyn mora</td>\n",
       "      <td>:na \\n</td>\n",
       "      <td>-&gt; First name ends with n, Last name ends wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455213</th>\n",
       "      <td>annie</td>\n",
       "      <td>hudson</td>\n",
       "      <td>annie hudson</td>\n",
       "      <td>:en \\n</td>\n",
       "      <td>-&gt; First name ends with e, Last name ends wit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         first      last          prompt reg_sol  \\\n",
       "311150    ruby   freeman    ruby freeman  :yn \\n   \n",
       "32142    logan  mcdonald  logan mcdonald  :nd \\n   \n",
       "120849    enzo  benjamin   enzo benjamin  :on \\n   \n",
       "471582  londyn      mora     londyn mora  :na \\n   \n",
       "455213   annie    hudson    annie hudson  :en \\n   \n",
       "\n",
       "                                                  cot_sol  \n",
       "311150   -> First name ends with y, Last name ends wit...  \n",
       "32142    -> First name ends with n, Last name ends wit...  \n",
       "120849   -> First name ends with o, Last name ends wit...  \n",
       "471582   -> First name ends with n, Last name ends wit...  \n",
       "455213   -> First name ends with e, Last name ends wit...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2182f7f2-8a06-4182-9707-bea7eb0d1ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' -> First name ends with y, Last name ends with n,so the solution is :yn \\n \\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[0].cot_sol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217a41e4-003a-4f40-8084-01df8c80df5e",
   "metadata": {},
   "source": [
    "Those are the few examples we will use for the generation of CoT and the regular prompting: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ea0537-76f7-4af0-be7e-bae2a581bcd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ximena medina -> First name ends with a, Last name ends with a,so the solution is :aa \n",
      " \n",
      "eli tapia -> First name ends with i, Last name ends with a,so the solution is :ia \n",
      " \n",
      "lilah collier -> First name ends with h, Last name ends with r,so the solution is :hr \n",
      " \n",
      "\n",
      "ximena medina:aa \n",
      "eli tapia:ia \n",
      "lilah collier:hr \n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_shot_cot = \"\".join(train.prompt + train.cot_sol)\n",
    "print(few_shot_cot)\n",
    "few_shot_reg = \"\".join(train.prompt + train.reg_sol)\n",
    "print(few_shot_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6325fae7-2114-4e68-bb85-c0fedb619096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.81it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.tqdm.pandas()\n",
    "results_reg=test.prompt.progress_apply(lambda x:infer(few_shot_reg+x,max_length=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09c857-ca54-4f2a-b09b-fe751ba398d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:38<00:00, 15.87s/it]\n"
     ]
    }
   ],
   "source": [
    "tqdm.tqdm.pandas()\n",
    "results_cot=test.prompt.progress_apply(lambda x:infer(few_shot_cot+x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4225e81f-86d8-444d-b535-80b809d55de2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gpt2_results = test.assign(label=test.reg_sol.str[1:4])[[\"first\",\"last\",\"label\"]].join(\n",
    "    results_cot.str[-2:].rename(\"cot\")\n",
    ").join(\n",
    "    results_reg.str.strip().str[-2:].rename(\"regular\")    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c710a203-141c-4c86-8cd3-9613219fc49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt2_results.to_csv(\"gpt2_results_cache.csv\")\n",
    "# gpt2_results = pd.read_csv(\"gpt2_results_cache.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cfb618-4957-4ae5-a14c-4bb3f2ee9921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>label</th>\n",
       "      <th>cot</th>\n",
       "      <th>regular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>311150</th>\n",
       "      <td>ruby</td>\n",
       "      <td>freeman</td>\n",
       "      <td>yn</td>\n",
       "      <td>an</td>\n",
       "      <td>hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32142</th>\n",
       "      <td>logan</td>\n",
       "      <td>mcdonald</td>\n",
       "      <td>nd</td>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120849</th>\n",
       "      <td>enzo</td>\n",
       "      <td>benjamin</td>\n",
       "      <td>on</td>\n",
       "      <td>\\nl</td>\n",
       "      <td>hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471582</th>\n",
       "      <td>londyn</td>\n",
       "      <td>mora</td>\n",
       "      <td>na</td>\n",
       "      <td>\\nl</td>\n",
       "      <td>ia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455213</th>\n",
       "      <td>annie</td>\n",
       "      <td>hudson</td>\n",
       "      <td>en</td>\n",
       "      <td>\\nl</td>\n",
       "      <td>hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329536</th>\n",
       "      <td>hailey</td>\n",
       "      <td>marsh</td>\n",
       "      <td>yh</td>\n",
       "      <td>sh</td>\n",
       "      <td>ia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36895</th>\n",
       "      <td>grayson</td>\n",
       "      <td>espinosa</td>\n",
       "      <td>na</td>\n",
       "      <td>\\nl</td>\n",
       "      <td>ia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438128</th>\n",
       "      <td>alexandra</td>\n",
       "      <td>herrera</td>\n",
       "      <td>aa</td>\n",
       "      <td>\\nl</td>\n",
       "      <td>hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366536</th>\n",
       "      <td>eden</td>\n",
       "      <td>marsh</td>\n",
       "      <td>nh</td>\n",
       "      <td>\\nl</td>\n",
       "      <td>ia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79000</th>\n",
       "      <td>easton</td>\n",
       "      <td>smith</td>\n",
       "      <td>nh</td>\n",
       "      <td>\\nl</td>\n",
       "      <td>hr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            first      last label   cot regular\n",
       "311150       ruby   freeman   yn     an      hr\n",
       "32142       logan  mcdonald   nd   \\n\\n      hr\n",
       "120849       enzo  benjamin   on    \\nl      hr\n",
       "471582     londyn      mora   na    \\nl      ia\n",
       "455213      annie    hudson   en    \\nl      hr\n",
       "329536     hailey     marsh   yh     sh      ia\n",
       "36895     grayson  espinosa   na    \\nl      ia\n",
       "438128  alexandra   herrera   aa    \\nl      hr\n",
       "366536       eden     marsh   nh    \\nl      ia\n",
       "79000      easton     smith   nh    \\nl      hr"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b011a9-6298-4526-8adc-9131d58e98b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>cot</th>\n",
       "      <th>regular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>295039</td>\n",
       "      <td>is</td>\n",
       "      <td>he</td>\n",
       "      <td>ae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>408372</td>\n",
       "      <td>nn</td>\n",
       "      <td>me</td>\n",
       "      <td>ae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>468311</td>\n",
       "      <td>hr</td>\n",
       "      <td>he</td>\n",
       "      <td>ae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14044</td>\n",
       "      <td>kl</td>\n",
       "      <td>j</td>\n",
       "      <td>ae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>192042</td>\n",
       "      <td>on</td>\n",
       "      <td>me</td>\n",
       "      <td>ae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>282918</td>\n",
       "      <td>es</td>\n",
       "      <td>me</td>\n",
       "      <td>eo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>169280</td>\n",
       "      <td>lr</td>\n",
       "      <td>he</td>\n",
       "      <td>eo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>376213</td>\n",
       "      <td>en</td>\n",
       "      <td>me</td>\n",
       "      <td>ae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>954</td>\n",
       "      <td>my</td>\n",
       "      <td>ny</td>\n",
       "      <td>ae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>448651</td>\n",
       "      <td>yl</td>\n",
       "      <td>th</td>\n",
       "      <td>ae</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 label cot regular\n",
       "0      295039   is   he      ae\n",
       "1      408372   nn   me      ae\n",
       "2      468311   hr   he      ae\n",
       "3       14044   kl    j      ae\n",
       "4      192042   on   me      ae\n",
       "5      282918   es   me      eo\n",
       "6      169280   lr   he      eo\n",
       "7      376213   en   me      ae\n",
       "8         954   my   ny      ae\n",
       "9      448651   yl   th      ae"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"gpt2_results_cache.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8885180c-5901-47bf-a87b-96cd365b8ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' -> First name ends with s, Last name ends with t,so the solution is :hr \\n\\nruby freeman -> Last name ends with s, Last name ends with t,so the solution is :hr \\n\\nruby freeman'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_cot.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da005757-5821-4ecb-a313-0892a92e79ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':hr '"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_reg.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6587fac7-ca91-44dd-8717-9bd95e7682e7",
   "metadata": {},
   "source": [
    "## Trying the experiment with PaLM 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b8e9d1-525d-44ff-9a0e-ebefb7544af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"api_key.txt\") as f:\n",
    "    palm.configure(api_key=f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bee245-c17d-4c1c-9c4b-4c7b3930a099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/text-bison-001\n"
     ]
    }
   ],
   "source": [
    "models = [m for m in palm.list_models() if 'generateText' in m.supported_generation_methods]\n",
    "model_name = models[0].name\n",
    "print(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5ceb50-fa9f-4199-b7ca-50e7dd7bd426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(name='models/text-bison-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='PaLM 2 (Legacy)',\n",
       "       description='A legacy model that understands text and generates text as an output',\n",
       "       input_token_limit=8196,\n",
       "       output_token_limit=1024,\n",
       "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
       "       temperature=0.7,\n",
       "       top_p=0.95,\n",
       "       top_k=40)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaf7c50-408a-4909-8165-e74b48563e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **Choose a plot of land.** The first step is to find a suitable plot of land for your home. This will depend on your budget, the size of the house you want, and the location you prefer.\n",
      "2. **Get the necessary permits and approvals.** Before you can start building your house, you will need to get the necessary permits and approvals from your local government. This process can take some time, so it's important to factor it into your timeline.\n",
      "3. **Design your house.** Once you have a plot of land, you can start designing your house. You will need to work with an architect or designer to create a plan that meets your needs and budget.\n",
      "4. **Build your foundation.** The foundation is the base of your house and it is essential to get it right. This is where the weight of the house will be supported, so it needs to be strong and stable.\n",
      "5. **Frame your house.** The frame of your house is made up of the walls, roof, and floor. This is the structure that will hold up the rest of the house.\n",
      "6. **Insulate your house.** Insulation helps to keep your house warm in the winter and cool in the summer. It is an important part of any home and should be installed before the walls are closed up.\n",
      "7. **Add windows and doors.** Windows and doors allow light and air into your house and provide access to the outside. They are an important part of the design and should be chosen carefully.\n",
      "8. **Install the roof.** The roof protects your house from the elements and is one of the most important parts of the structure. It should be installed by a qualified professional.\n",
      "9. **Finish the interior.** This includes things like painting the walls, installing flooring, and adding cabinets and countertops. You can choose to do this yourself or hire a contractor to help you.\n",
      "10. **Move in!** Once your house is finished, it's time to move in and enjoy your new home.\n"
     ]
    }
   ],
   "source": [
    "completion = palm.generate_text(\n",
    "    model=model_name,\n",
    "    prompt=\"How does one build a house?\",\n",
    "    temperature=0,\n",
    "    # The maximum length of the response\n",
    "    max_output_tokens=800,\n",
    ")\n",
    "\n",
    "print(completion.result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13428fa-3f61-4a77-bcf4-22c6f97a7c1e",
   "metadata": {},
   "source": [
    "### proof that the model is not working in regular FewShot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e6ce42-1b81-4876-a23b-5935c03cbae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jb'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palm.generate_text(\n",
    "    model=model_name,\n",
    "    prompt=\"Do a last letter abriviation: Avi Hirsch-> ih \\n Yoni Mendel -> il \\n Johny Boy -> \",\n",
    "    temperature=0,\n",
    "    # The maximum length of the response\n",
    "    max_output_tokens=800,\n",
    ").result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68493967-3c5c-45a2-be4d-6113838aad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "palm_gen = lambda x:palm.generate_text(\n",
    "    model=model_name,\n",
    "    prompt=x,\n",
    "    temperature=0,\n",
    "    # The maximum length of the response\n",
    "    max_output_tokens=800,\n",
    ").result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc15c99b-af5a-4d5f-8140-58655d8c822d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.50it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.tqdm.pandas()\n",
    "results_reg_palm=test.prompt.progress_apply(lambda x:palm_gen(few_shot_reg + x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b190a320-9e77-46c9-a5ea-84d2837c1054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.76it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.tqdm.pandas()\n",
    "results_cot_palm=test.prompt.progress_apply(lambda x:palm_gen(few_shot_cot+x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63c2b4e-c1fd-4e40-9397-8012d1a4b1c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "palm_results = test.assign(label=test.reg_sol.str[1:4])[[\"label\",\"prompt\"]].join(\n",
    "    results_cot_palm.str[-2:].rename(\"cot\")\n",
    ").join(\n",
    "    results_reg_palm.str[-2:].rename(\"regular\")    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd63883-3bad-4803-977c-43be0923a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# palm_results.to_csv(\"palm_results_cache2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35bfa67-ee40-4ac1-94f4-65bd4d6b0ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>prompt</th>\n",
       "      <th>cot</th>\n",
       "      <th>regular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>311150</th>\n",
       "      <td>yn</td>\n",
       "      <td>ruby freeman</td>\n",
       "      <td>yn</td>\n",
       "      <td>rr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32142</th>\n",
       "      <td>nd</td>\n",
       "      <td>logan mcdonald</td>\n",
       "      <td>nd</td>\n",
       "      <td>rr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120849</th>\n",
       "      <td>on</td>\n",
       "      <td>enzo benjamin</td>\n",
       "      <td>on</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471582</th>\n",
       "      <td>na</td>\n",
       "      <td>londyn mora</td>\n",
       "      <td>na</td>\n",
       "      <td>hr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455213</th>\n",
       "      <td>en</td>\n",
       "      <td>annie hudson</td>\n",
       "      <td>in</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329536</th>\n",
       "      <td>yh</td>\n",
       "      <td>hailey marsh</td>\n",
       "      <td>yh</td>\n",
       "      <td>ia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36895</th>\n",
       "      <td>na</td>\n",
       "      <td>grayson espinosa</td>\n",
       "      <td>na</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438128</th>\n",
       "      <td>aa</td>\n",
       "      <td>alexandra herrera</td>\n",
       "      <td>aa</td>\n",
       "      <td>ia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366536</th>\n",
       "      <td>nh</td>\n",
       "      <td>eden marsh</td>\n",
       "      <td>None</td>\n",
       "      <td>ia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79000</th>\n",
       "      <td>nh</td>\n",
       "      <td>easton smith</td>\n",
       "      <td>nh</td>\n",
       "      <td>rr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label             prompt   cot regular\n",
       "311150   yn        ruby freeman    yn      rr\n",
       "32142    nd      logan mcdonald    nd      rr\n",
       "120849   on       enzo benjamin    on    None\n",
       "471582   na         londyn mora    na      hr\n",
       "455213   en        annie hudson    in    None\n",
       "329536   yh        hailey marsh    yh      ia\n",
       "36895    na    grayson espinosa    na    None\n",
       "438128   aa   alexandra herrera    aa      ia\n",
       "366536   nh          eden marsh  None      ia\n",
       "79000    nh        easton smith    nh      rr"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7870ecb-6071-48af-8d5c-ae760fb4ad81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cot</th>\n",
       "      <th>regular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>130676</th>\n",
       "      <td>nd</td>\n",
       "      <td>rd</td>\n",
       "      <td>ey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102356</th>\n",
       "      <td>tn</td>\n",
       "      <td>tn</td>\n",
       "      <td>ey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419932</th>\n",
       "      <td>ne</td>\n",
       "      <td>ne</td>\n",
       "      <td>ey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22860</th>\n",
       "      <td>nf</td>\n",
       "      <td>nn</td>\n",
       "      <td>ey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71813</th>\n",
       "      <td>er</td>\n",
       "      <td>er</td>\n",
       "      <td>ey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418635</th>\n",
       "      <td>ha</td>\n",
       "      <td>ha</td>\n",
       "      <td>ey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320855</th>\n",
       "      <td>rr</td>\n",
       "      <td>rs</td>\n",
       "      <td>ey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155106</th>\n",
       "      <td>rn</td>\n",
       "      <td>rn</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315459</th>\n",
       "      <td>nz</td>\n",
       "      <td>nz</td>\n",
       "      <td>ey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105672</th>\n",
       "      <td>es</td>\n",
       "      <td>et</td>\n",
       "      <td>ey</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label cot regular\n",
       "130676   nd   rd      ey\n",
       "102356   tn   tn      ey\n",
       "419932   ne   ne      ey\n",
       "22860    nf   nn      ey\n",
       "71813    er   er      ey\n",
       "418635   ha   ha      ey\n",
       "320855   rr   rs      ey\n",
       "155106   rn   rn       m\n",
       "315459   nz   nz      ey\n",
       "105672   es   et      ey"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"palm_results_cache.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a210226c-2f45-40a6-b113-eb26b8fdb34f",
   "metadata": {},
   "source": [
    "# Article results for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e9eb87-8e17-46d3-93ec-0b3140bf402a",
   "metadata": {},
   "source": [
    "The following graph shows the performance of the models in the paper. It compares standard with COF prompting on the letter concatenation and the coin flip experiment. You can see results for in and out of the training domain: whether the training examples were similar to the testing example in amount of coin flips and amount of words in each name. You can also see the results for models of different size (different version of PaLM).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b97378-013d-4d03-ab91-87bd87ba00cc",
   "metadata": {},
   "source": [
    "![articleres](articleres.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7253cbb-1d58-4d9d-803c-e4411a343fe8",
   "metadata": {},
   "source": [
    "# What next? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa43f78b-a909-4b0b-ab77-ccfd5f114ba2",
   "metadata": {},
   "source": [
    "You can learn much more about prompt engineering and this presentation only cover the tip of the iceberg. To get a better understanding of the research that was conducted on the topic during the last 2 years you can check the following survey article: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a267f09d-f3bc-48da-932e-f74a82966197",
   "metadata": {},
   "source": [
    "[A Systematic Survey of Prompt Engineering in Large Language Models:\n",
    "Techniques and Applications](https://arxiv.org/pdf/2402.07927)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7679ada0-2ef6-49f0-9c42-6374b6f4e58b",
   "metadata": {},
   "source": [
    "![pes](promptengineeringsurvey.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b714ac42-8367-452b-ab66-17ae14a79cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
