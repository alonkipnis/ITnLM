{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords Extraction from Emails via Entropy Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation for Keywords/Key-phrases extraction within emails:\n",
    "- Email traffic is really high and it would be helpful to get a general idea of what the email is about without going through it.\n",
    "- Key phrases can offer an easy form of summarization - they convey the main information within the email.\n",
    "- Key phrases and keywords can be used later on to filter emails into categories or for retrieval of similar emails based on keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "The quality of the results during the workshop are incremental i.e. - they will improve as it progresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing methods for keywords extraction\n",
    "- RAKE: partitions documents by stop-words and word/phrase delimiters into candidate keywords. \n",
    "Then computes co-occurrence matrix and based on the matrix gives a score for each word(deg/freq) - the it selects top n scores of summed sequences.\n",
    "Overall efficient and light. \n",
    "https://www.researchgate.net/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents\n",
    "\n",
    "- YAKE!: \n",
    "A light-weight unsupervised automatic keyword extraction method which rests on text statistical features extracted from single documents to select the most important keywords of a text.\n",
    "\n",
    "- TF-IDF - self-explanatory.\n",
    "\n",
    "- Key-Bert computes sentence embedding and then finds ngrams which has the highest cosine similarity to the sentence embedding.\n",
    "\n",
    "- Key-Bert with POS pre matching - same, but instead od specifying ngrams you specify POS matchers for phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AESLC -  Annotated Enron Subject Line Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: python: command not found\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting datasets\n",
      "  Downloading datasets-2.8.0-py3-none-any.whl (452 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.11.1\n",
      "  Downloading fsspec-2022.11.0-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.5/139.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-10.0.1-cp38-cp38-macosx_10_14_x86_64.whl (25.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.0/25.0 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/site-packages (from datasets) (5.4.1)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/site-packages (from datasets) (1.2.4)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.2.0\n",
      "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/site-packages (from datasets) (3.7.3)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp38-cp38-macosx_10_9_x86_64.whl (35 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/site-packages (from datasets) (1.22.2)\n",
      "Collecting dill<0.3.7\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.62.1\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.8/site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Collecting chardet<4.0,>=2.0\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.8/site-packages (from aiohttp->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: chardet, xxhash, tqdm, pyarrow, fsspec, dill, responses, multiprocess, huggingface-hub, datasets\n",
      "  Attempting uninstall: chardet\n",
      "    Found existing installation: chardet 4.0.0\n",
      "    Uninstalling chardet-4.0.0:\n",
      "      Successfully uninstalled chardet-4.0.0\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.59.0\n",
      "    Uninstalling tqdm-4.59.0:\n",
      "      Successfully uninstalled tqdm-4.59.0\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 2.0.0\n",
      "    Uninstalling pyarrow-2.0.0:\n",
      "      Successfully uninstalled pyarrow-2.0.0\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 0.8.7\n",
      "    Uninstalling fsspec-0.8.7:\n",
      "      Successfully uninstalled fsspec-0.8.7\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.0.12\n",
      "    Uninstalling huggingface-hub-0.0.12:\n",
      "      Successfully uninstalled huggingface-hub-0.0.12\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "twine 3.8.0 requires urllib3>=1.26.0, but you have urllib3 1.25.11 which is incompatible.\n",
      "transformers 4.9.2 requires huggingface-hub==0.0.12, but you have huggingface-hub 0.11.1 which is incompatible.\n",
      "kedro 0.17.1 requires fsspec<0.9,>=0.5.1, but you have fsspec 2022.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed chardet-3.0.4 datasets-2.8.0 dill-0.3.6 fsspec-2022.11.0 huggingface-hub-0.11.1 multiprocess-0.70.14 pyarrow-10.0.1 responses-0.18.0 tqdm-4.64.1 xxhash-3.2.0\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "!python -m spacy download en_core_web_sm\n",
    "%pip install datasets\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cba9af06c444af9b80e96f10f8c48e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214eaf8c1c074a7b8838c49a981e775a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baec289960e3461f8165f600695c59e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/6.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'email_body': Value(dtype='string', id=None),\n",
       " 'subject_line': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset_builder\n",
    "ds_builder = load_dataset_builder(\"aeslc\")\n",
    "\n",
    "ds_builder.info.description\n",
    "\n",
    "ds_builder.info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset aeslc/default to /Users/kipnisal/.cache/huggingface/datasets/aeslc/default/1.0.0/eb8e30234cf984a58ebe9f205674597ac1db2ec91e7321cd7f36864f7e3671b8...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f755adb0970f4c998032c3d259287ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52336cc533fd41aab79ff9f50ac15e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing checksums:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8112ec2ed34268983e96a9b8197fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14436 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e9579e12ef4a58985d75d9d5d9cea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1960 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f96315ac4e49e3a9f0cc9348cf3152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1906 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset aeslc downloaded and prepared to /Users/kipnisal/.cache/huggingface/datasets/aeslc/default/1.0.0/eb8e30234cf984a58ebe9f205674597ac1db2ec91e7321cd7f36864f7e3671b8. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cabf14bec984bd6807d02e38265bfdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"aeslc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example for keywords from email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greg/Phillip,  Attached is the Grande Communications Service Agreement.\n",
      "The business points can be found in Exhibit C.  I Can get the Non-Disturbance agreement after it has been executed by you and Grande.\n",
      "I will fill in the Legal description of the property one I have received it.\n",
      "Please execute and send to:  Grande Communications, 401 Carlson Circle, San Marcos Texas, 78666 Attention Hunter Williams.\n",
      "<<Bishopscontract.doc>>\n",
      "\n",
      "Possible Keywords:\n",
      "Service Agreement\n"
     ]
    }
   ],
   "source": [
    "# Select a random  sample from the training set\n",
    "import random\n",
    "# for i in random.sample(range(0, len(dataset[\"train\"])), 3):\n",
    "#     print(dataset[\"train\"][i][\"email_body\"])\n",
    "\n",
    "print(dataset[\"train\"][0][\"email_body\"])\n",
    "print(\"Possible Keywords:\")\n",
    "print(dataset[\"train\"][0][\"subject_line\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2\n",
    "\n",
    "- Trained on WebText Dataset: filtered outbound links from reddit which received at least 3 karma( heuristic indicator for whether other users found the link interesting, educational,or just funny).\n",
    "- 355M parameters, similar largest original BERT (GPT-3 in comparison 175 BN parameters)\n",
    "- Byte Pair encoding tokenizer (vocab size of 50257) - https://towardsdatascience.com/comparing-transformer-tokenizers-686307856955\n",
    "- Trained on 512 batch size\n",
    "- Input size 1024 tokens\n",
    "- Similar to original GPT and any decoder Transformer in architecture.\n",
    "- Can be used to evaluate entropy since it's auto-regressive - attends to past (similar to information flow)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Card: https://huggingface.co/gpt2-medium\n",
    "- Paper: https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "\n",
    "# This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n",
    "# be encoded differently whether it is at the beginning of the sentence (without space) or not\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2-medium')\n",
    "\n",
    "# 355M parameter version of GPT-2 which was trained with language modeling objective\n",
    "# Has a language modeling head on top\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity of auto-regressive LLMs:\n",
    "\n",
    "- GPT2 has been trained with minimization of cross entropy loss on next token prediction(which is the as same max likelihood)\n",
    "- This can be viewed as training on minimizing the perplexity/cross entropy - i.e. predicting the next most probable token /encoding the language in the most efficient manner\n",
    "- Perplexity score(base 2) - confusion of the model, e.g perplexity 10 means that the model is confident in the word as if it had to choose it uniformly from 10 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average word entropy per email based on email length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_body(email_body, tokenizer, is_batched=False):\n",
    "    if is_batched:\n",
    "        email_body = [normalize_body(email, tokenizer, is_batched=False) for email in email_body]\n",
    "    else:\n",
    "        email_body = email_body.replace(\"\\t\", \" \")\n",
    "        # Adding bos token to the beginning of each email\n",
    "        email_body = tokenizer.bos_token + \" \".join(email_body.split())\n",
    "\n",
    "    return email_body\n",
    "\n",
    "\n",
    "def tokenize_emails(dataset, tokenizer, should_use_padding=True, should_only_normalize=False):\n",
    "    dataset = dataset.map(\n",
    "        lambda batch: {\n",
    "            \"email_body\": normalize_body(batch[\"email_body\"], tokenizer, is_batched=True)\n",
    "        },\n",
    "        batched=True,\n",
    "    )\n",
    "    if should_only_normalize:\n",
    "        return dataset\n",
    "    dataset.set_format(type=\"torch\")\n",
    "    # Set a padding token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Batch tokenize the dataset\n",
    "    tokenized_emails = dataset.map(\n",
    "        lambda x: {\n",
    "            **tokenizer(\n",
    "                x[\"email_body\"],\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=\"max_length\" if should_use_padding else False,\n",
    "                add_special_tokens=True,\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # set format of the dataset to pytorch tensors\n",
    "\n",
    "    return tokenized_emails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\tsale\\.cache\\huggingface\\datasets\\aeslc\\default\\1.0.0\\eb8e30234cf984a58ebe9f205674597ac1db2ec91e7321cd7f36864f7e3671b8\\cache-29fe0f440707d9ac.arrow\n",
      "Loading cached processed dataset at C:\\Users\\tsale\\.cache\\huggingface\\datasets\\aeslc\\default\\1.0.0\\eb8e30234cf984a58ebe9f205674597ac1db2ec91e7321cd7f36864f7e3671b8\\cache-2c8a3fbb292a3fcb.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_emails_non_padded = tokenize_emails(dataset[\"train\"], tokenizer, should_use_padding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\tsale\\.cache\\huggingface\\datasets\\aeslc\\default\\1.0.0\\eb8e30234cf984a58ebe9f205674597ac1db2ec91e7321cd7f36864f7e3671b8\\cache-f7d252dd4dcd1df9.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 95th percentile of token length of tokenized emails: \n",
      "451.0\n",
      "Median of tokenized emails: \n",
      "96.5\n"
     ]
    }
   ],
   "source": [
    "# Compute percentiles of tokenized emails\n",
    "import numpy as np\n",
    "\n",
    "length_dataset = tokenized_emails_non_padded.map(lambda x: {\"len\" :len(x[\"input_ids\"][0])})\n",
    "# tokenized_emails_lengths = [len(tokenized_emails_non_padded[\"input_ids\"][0]) for tokenized_email in tokenized_emails_non_padded]\n",
    "print(\" 95th percentile of token length of tokenized emails: \")\n",
    "# print(np.percentile(tokenized_emails_lengths, 95))\n",
    "print(np.percentile(length_dataset[\"len\"], 95))\n",
    "# Average length of tokenized emails\n",
    "print(\"Median of tokenized emails: \")\n",
    "# print(np.mean(tokenized_emails_lengths))\n",
    "print(np.percentile(length_dataset[\"len\"], 50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot distribution of tokenized emails length\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_distribution(dataset, title):\n",
    "    plt.hist(dataset[\"len\"], bins=100)\n",
    "    plt.title(title)\n",
    "    # Set y axis label\n",
    "    plt.ylabel(\"Number of emails\")\n",
    "    # Set x axis label\n",
    "    plt.xlabel(\"Length of tokenized emails\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_distribution(length_dataset, \"Distribution of tokenized emails length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\tsale\\.cache\\huggingface\\datasets\\aeslc\\default\\1.0.0\\eb8e30234cf984a58ebe9f205674597ac1db2ec91e7321cd7f36864f7e3671b8\\cache-659ab0a602d2b399.arrow\n",
      "Loading cached processed dataset at C:\\Users\\tsale\\.cache\\huggingface\\datasets\\aeslc\\default\\1.0.0\\eb8e30234cf984a58ebe9f205674597ac1db2ec91e7321cd7f36864f7e3671b8\\cache-0746aca2abf48bb0.arrow\n"
     ]
    }
   ],
   "source": [
    "padded_train = tokenize_emails(dataset[\"train\"], tokenizer, should_only_normalize=False, should_use_padding=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It seems that average entropy per token is going down as emails are longer\n",
    "- Context of longer emails gives the model more confidence on predictions.\n",
    "- Larger emails contain more redundancy and can be encoded more efficiently on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy_from_logits(logits , relevant_tokens ):\n",
    "    probs = logits[0].softmax(dim=-1)\n",
    "\n",
    "    # Get the actual probs of each token\n",
    "    actual_probs = probs[range(len(probs) -1), relevant_tokens]\n",
    "\n",
    "    # Get cross entropy of each token\n",
    "    entropy = -actual_probs.log2()\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_entropy(model, tokenized_email):\n",
    "\n",
    "    # Convert model to cpu\n",
    "    model.cpu()\n",
    "    # Get probabilities from logits\n",
    "    logits = model(**tokenized_email).logits\n",
    "\n",
    "    # Remove bos token from tokenized email\n",
    "    relevant_tokens = tokenized_email[\"input_ids\"][0][1:]\n",
    "\n",
    "    entropy = calculate_entropy_from_logits(logits, relevant_tokens)\n",
    "\n",
    "    return entropy , relevant_tokens, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_email():\n",
    "    email = dataset[\"train\"][random.randint(0, len(dataset[\"train\"]))]\n",
    "    email_body = email[\"email_body\"]\n",
    "    email_body = email_body.replace(\"\\t\", \" \")\n",
    "    email_body = \" \".join(email_body.split())\n",
    "    return email_body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_single_email(email_body, tokenizer):\n",
    "    tokenized_email = tokenizer(\n",
    "        tokenizer.bos_token + email_body,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    return tokenized_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_entropy_tokens(entropy, relevant_tokens, n):\n",
    "    # Select the n tokens with the highest entropy\n",
    "    top_n_elements = entropy.topk(n, dim=-1).indices\n",
    "\n",
    "    # Take first n elements\n",
    "    top_n_elements = top_n_elements[:n]\n",
    "\n",
    "    # Select these tokens from the original tokenized email\n",
    "    top_n_tokens = relevant_tokens[top_n_elements]\n",
    "\n",
    "    return top_n_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_and_print_tokens_with_entropy(relevant_tokens, entropy, should_display_entropy=True):\n",
    "    decoded = tokenizer.batch_decode(relevant_tokens)\n",
    "\n",
    "    if should_display_entropy:\n",
    "        token_entropy = list(zip(decoded, entropy.tolist(), range(len(relevant_tokens))))\n",
    "    else :\n",
    "        token_entropy = list(zip(decoded, range(len(relevant_tokens))))\n",
    "\n",
    "    print(token_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_indices(some_list, k):\n",
    "    # Convert to numpy array\n",
    "    some_list = np.array(some_list)\n",
    "    # Get the indices of the top k values\n",
    "    top_k_indices = some_list.argsort()[-k:][::-1]\n",
    "    # return as list\n",
    "    return top_k_indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_pairs_of_highest_entropy(entropy, decoded, k, n):\n",
    "    entropy_pairs = []\n",
    "    entropy_pairs_tokens = []\n",
    "    # Fill tensor with average entropy of each k pair\n",
    "    for i in range(len(entropy) - n):\n",
    "        entropy_pairs.append(entropy[i:i+n].mean().detach().numpy())\n",
    "        entropy_pairs_tokens.append(decoded[i:i+n])\n",
    "\n",
    "    # Choose the n pairs with the highest average entropy\n",
    "    top_n_pairs = get_top_k_indices(entropy_pairs, k)\n",
    "\n",
    "    # Get original \n",
    "    top_n_pairs_tokens = [entropy_pairs_tokens[i] for i in top_n_pairs]\n",
    "\n",
    "    return top_n_pairs_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get keywords directly from Email By Cross-Entropy(log loss)\n",
    "- Tokenize email\n",
    "- Calculate cross entropy of each token(which is an upper bound to the real entropy)\n",
    "- Select the n words with the highest entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I spoke to Kate Cole and she said Enron NetWorks (I'm not sure of the correct spelling yet) was incorporated as a Delaware corporation on 4/13/00. She has made me a copy of their articles of incorporation and LLC Agreement and I am sending a messenger to pick them up, as well as a copy of the officers and directors, which appears to have been approved. The banking resolution went to Julia today for signature. Do we need to adopt any other resolutions (brokerage, approval of agreements, etc.). How about adoption of the trading policy?\n"
     ]
    }
   ],
   "source": [
    "email_body =sample_email()\n",
    "print(email_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the email: 117\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the email with  bos token at the beginning and eos token at the end\n",
    "tokenized_email = tokenize_single_email(email_body, tokenizer)\n",
    "\n",
    "num_of_tokens = len(tokenized_email[\"input_ids\"][0])\n",
    "print(f\"Number of tokens in the email: {num_of_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log cross Entropy(log loss) of original email by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 5.774309158325195, 0), (' spoke', 10.583921432495117, 1), (' to', 1.4048041105270386, 2), (' Kate', 10.267398834228516, 3), (' Cole', 12.83829116821289, 4), (' and', 5.220125675201416, 5), (' she', 5.7715630531311035, 6), (' said', 3.98419451713562, 7), (' En', 16.947742462158203, 8), ('ron', 4.708259582519531, 9), (' Net', 18.046255111694336, 10), ('Works', 10.68776798248291, 11), (' (', 5.969388484954834, 12), ('I', 9.096165657043457, 13), (\"'m\", 3.6776819229125977, 14), (' not', 2.0491461753845215, 15), (' sure', 1.1812381744384766, 16), (' of', 5.57489013671875, 17), (' the', 0.5626698136329651, 18), (' correct', 4.9640960693359375, 19), (' spelling', 1.1721925735473633, 20), (' yet', 7.790988445281982, 21), (')', 0.6072407960891724, 22), (' was', 1.554545521736145, 23), (' incorporated', 10.50373363494873, 24), (' as', 4.5574164390563965, 25), (' a', 1.1972997188568115, 26), (' Delaware', 6.993647575378418, 27), (' corporation', 1.1859043836593628, 28), (' on', 3.9812917709350586, 29), (' 4', 7.57173490524292, 30), ('/', 0.511001706123352, 31), ('13', 5.148547172546387, 32), ('/', 0.13491761684417725, 33), ('00', 6.821066379547119, 34), ('.', 1.0962480306625366, 35), (' She', 3.6210145950317383, 36), (' has', 5.5365681648254395, 37), (' made', 8.013073921203613, 38), (' me', 7.157547473907471, 39), (' a', 3.752781629562378, 40), (' copy', 2.259594202041626, 41), (' of', 0.14308764040470123, 42), (' their', 4.44523811340332, 43), (' articles', 10.408978462219238, 44), (' of', 6.399141311645508, 45), (' incorporation', 0.08115722984075546, 46), (' and', 2.4130959510803223, 47), (' LLC', 12.136731147766113, 48), (' Agreement', 8.764375686645508, 49), (' and', 3.4340832233428955, 50), (' I', 3.360795259475708, 51), (' am', 3.6171228885650635, 52), (' sending', 5.485340595245361, 53), (' a', 5.24299430847168, 54), (' messenger', 18.29714584350586, 55), (' to', 1.120090126991272, 56), (' pick', 8.126031875610352, 57), (' them', 2.243654489517212, 58), (' up', 0.021687109023332596, 59), (',', 6.002824306488037, 60), (' as', 3.785884380340576, 61), (' well', 3.688159704208374, 62), (' as', 0.2212386429309845, 63), (' a', 2.9747314453125, 64), (' copy', 0.6619274020195007, 65), (' of', 0.08623731881380081, 66), (' the', 1.3564425706863403, 67), (' officers', 13.325668334960938, 68), (' and', 2.539902687072754, 69), (' directors', 0.40122029185295105, 70), (',', 6.002699851989746, 71), (' which', 4.363633632659912, 72), (' appears', 9.181000709533691, 73), (' to', 0.5157884955406189, 74), (' have', 3.738433837890625, 75), (' been', 0.5989115238189697, 76), (' approved', 5.720558166503906, 77), ('.', 3.7425460815429688, 78), (' The', 4.406452655792236, 79), (' banking', 15.24044132232666, 80), (' resolution', 13.335633277893066, 81), (' went', 9.67692756652832, 82), (' to', 3.125654935836792, 83), (' Julia', 17.965314865112305, 84), (' today', 15.772771835327148, 85), (' for', 5.707277297973633, 86), (' signature', 3.7158288955688477, 87), ('.', 1.0769294500350952, 88), (' Do', 9.05968189239502, 89), (' we', 4.858018398284912, 90), (' need', 2.8044204711914062, 91), (' to', 0.9158090949058533, 92), (' adopt', 10.89726734161377, 93), (' any', 5.1378583908081055, 94), (' other', 3.782749652862549, 95), (' resolutions', 5.964349269866943, 96), (' (', 7.498824596405029, 97), ('bro', 12.012710571289062, 98), ('ker', 0.6829995512962341, 99), ('age', 0.6814222931861877, 100), (',', 1.2407934665679932, 101), (' approval', 11.151063919067383, 102), (' of', 2.051086664199829, 103), (' agreements', 9.403817176818848, 104), (',', 1.5075112581253052, 105), (' etc', 0.3329817056655884, 106), ('.).', 5.802828311920166, 107), (' How', 7.625157356262207, 108), (' about', 3.7847366333007812, 109), (' adoption', 8.877193450927734, 110), (' of', 0.6083789467811584, 111), (' the', 2.4393105506896973, 112), (' trading', 12.625184059143066, 113), (' policy', 7.518049240112305, 114), ('?', 1.9001057147979736, 115)]\n"
     ]
    }
   ],
   "source": [
    "should_display_entropy = True\n",
    "\n",
    "entropy , relevant_tokens, logits = get_tokens_entropy(model, tokenized_email)\n",
    "\n",
    "decode_and_print_tokens_with_entropy(relevant_tokens, entropy, should_display_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 0), (' enable', 1), (' the', 2), (' following', 3), (' PDF', 4), (' release', 5), ('.', 6), (' which', 7), (' includes', 8), (' will', 9), (' like', 10), (' to', 11), (' share', 12), (' to', 13), (' soon', 14), (' as', 15), (' possible', 16), ('.', 17), (' morning', 18), ('.', 19), ('\\n', 20), (' note', 21), (' that', 22), (' you', 23), (' have', 24), (' received', 25), (' in', 26), (' fact', 27), (',', 28), (' received', 29), (' the', 30), (' the', 31), (' project', 32), ('1', 33), (' million', 34), (' deal', 35), (' $', 36), (' that', 37), (' us', 38), (' know', 39), (' if', 40), (' you', 41), (' have', 42), (' any', 43), (' questions', 44), (' questions', 45), ('.', 46), ('\\n', 47), (' you', 48), ('.', 49), ('\\n', 50)]\n"
     ]
    }
   ],
   "source": [
    "# Get argmax of the logits\n",
    "argmax = logits.argmax(dim=-1)\n",
    "\n",
    "\n",
    "# Decode these tokens\n",
    "actual_tokens_decoded = tokenizer.batch_decode(argmax[0])\n",
    "\n",
    "# Print the actual tokens\n",
    "print(list(zip(actual_tokens_decoded, range(len(actual_tokens_decoded)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I spoke to Kate Cole and she said Enron NetWorks (I'm not sure of the correct spelling yet) was incorporated as a Delaware corporation on 4/13/00. She has made me a copy of their articles of incorporation and LLC Agreement and I am sending a messenger to pick them up, as well as a copy of the officers and directors, which appears to have been approved. The banking resolution went to Julia today for signature. Do we need to adopt any other resolutions (brokerage, approval of agreements, etc.). How about adoption of the trading policy?\n"
     ]
    }
   ],
   "source": [
    "# print email body\n",
    "print(email_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs of tokens with the highest entropy:\n",
      "[[' approved', '.', ' The', ' banking', ' resolution', ' went', ' to', ' Julia', ' today', ' for']]\n"
     ]
    }
   ],
   "source": [
    "# ngram length\n",
    "n = 10\n",
    "# number of pairs to print \n",
    "k = 1\n",
    "decoded = tokenizer.batch_decode(relevant_tokens)\n",
    "print(\"Pairs of tokens with the highest entropy:\")\n",
    "print(get_k_pairs_of_highest_entropy(entropy, decoded, k, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting key-phrases with maximal entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs of tokens with the highest entropy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[' En', 'ron', ' Net'],\n",
       " [' Julia', ' today', ' for'],\n",
       " [' banking', ' resolution', ' went']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ngram length\n",
    "n = 3\n",
    "# number of pairs to print \n",
    "k = 3\n",
    "decoded = tokenizer.batch_decode(relevant_tokens)\n",
    "print(\"Pairs of tokens with the highest entropy:\")\n",
    "get_k_pairs_of_highest_entropy(entropy, decoded, k, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use KeyBert to extract keywords\n",
    "- document embeddings are extracted with BERT to get a document-level representation.  word embeddings are extracted for N-gram words/phrases.use cosine similarity to find the words/phrases that are the most similar to the document. The most similar words could then be identified as the words that best describe the entire document.\n",
    "\n",
    "- Keybert github: https://github.com/MaartenGr/KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('enron', 0.4661), ('incorporation', 0.4312), ('incorporated', 0.3569), ('corporation', 0.3479), ('agreements', 0.345)]\n"
     ]
    }
   ],
   "source": [
    "# Import keybert\n",
    "from keybert import KeyBERT\n",
    "\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(email_body, keyphrase_ngram_range=(1, 1), stop_words=None)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('delaware corporation on', 0.5408), ('enron networks not', 0.5218), ('as delaware corporation', 0.5194), ('resolutions brokerage approval', 0.503), ('enron networks', 0.5017)]\n"
     ]
    }
   ],
   "source": [
    "keywords = kw_model.extract_keywords(email_body, keyphrase_ngram_range=(1, n), stop_words=None)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanations for results:\n",
    "\n",
    " - Early token curse(most of the perplexity is assigned to the early tokens due to lack of context)\n",
    " - Distribution shift of email domain - higher perplexity score on email specific terms.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salient Sentences via average cross-entropy(log loss) of sentence\n",
    "\n",
    "## Motivation\n",
    "- Sentences with high cross-entropy(log loss) encode important information\n",
    "- Focusing on such sentences can yield high quality keywords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've located the Gas Nomination information that was prepared for Dec. 01 and updated to include Jan. 02 business. The Dec.01 numbers differ slightly from the initial numbers generated by James Centilli at TW as well as the numbers that Stephanie Miller conveyed in her Dec. 18th memo. These values are based on the nomination worksheets & actual power bills from he respective utilities. Thanks\n"
     ]
    }
   ],
   "source": [
    "email_body =sample_email()\n",
    "print(email_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I've located the Gas Nomination information that was prepared for Dec. 01 \"\n",
      " 'and updated to include Jan. 02 business.',\n",
      " 'The Dec.01 numbers differ slightly from the initial numbers generated by '\n",
      " 'James Centilli at TW as well as the numbers that Stephanie Miller conveyed '\n",
      " 'in her Dec. 18th memo.',\n",
      " 'These values are based on the nomination worksheets & actual power bills '\n",
      " 'from he respective utilities.']\n"
     ]
    }
   ],
   "source": [
    "# import nltk sent_tokenize\n",
    "# import pretty print\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# split the email body into sentences\n",
    "sentences = sent_tokenize(email_body)\n",
    "\n",
    "# Remove sentences that have less than 5 words\n",
    "sentences = [sentence for sentence in sentences if len(sentence.split()) > 5]\n",
    "pprint(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highest_entropy_sentence(sentences, model, tokenizer):\n",
    "# For each sentence compute the average entropy of the tokens\n",
    "    sentences_entropy = []\n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence\n",
    "        tokenized_sentence = tokenizer(tokenizer.bos_token + sentence, return_tensors=\"pt\", add_special_tokens = True)\n",
    "        # Get the entropy of the tokens\n",
    "        sentence_tokens_entropy, relevant, _ = get_tokens_entropy(model, tokenized_sentence)\n",
    "        # Get the average entropy of the tokens\n",
    "        sentence_entropy = sentence_tokens_entropy.mean()\n",
    "        sentences_entropy.append(sentence_entropy)\n",
    "\n",
    "    # Get the sentence with the highest entropy\n",
    "    most_uncertain_sentence = sentences[sentences_entropy.index(max(sentences_entropy))]\n",
    "    return most_uncertain_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('These values are based on the nomination worksheets & actual power bills '\n",
      " 'from he respective utilities.')\n"
     ]
    }
   ],
   "source": [
    "most_uncertain_sentence = get_highest_entropy_sentence(sentences, model, tokenizer)\n",
    "pprint(most_uncertain_sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('These', 9.314399719238281, 0), (' values', 11.722175598144531, 1), (' are', 1.1598787307739258, 2), (' based', 4.898871898651123, 3), (' on', 0.12823066115379333, 4), (' the', 1.9730063676834106, 5), (' nomination', 16.9766902923584, 6), (' works', 13.12839126586914, 7), ('he', 2.380789041519165, 8), ('ets', 2.682954254851211e-05, 9), (' &', 12.118532180786133, 10), (' actual', 11.560111045837402, 11), (' power', 11.842788696289062, 12), (' bills', 10.608786582946777, 13), (' from', 3.9716379642486572, 14), (' he', 15.849015235900879, 15), (' respective', 10.020634651184082, 16), (' utilities', 8.702932357788086, 17), ('.', 1.0877732038497925, 18)]\n"
     ]
    }
   ],
   "source": [
    "should_display_entropy = True\n",
    "\n",
    "tokenized_most_uncertain_sentence = tokenizer(tokenizer.bos_token + most_uncertain_sentence, return_tensors=\"pt\", add_special_tokens = True)\n",
    "\n",
    "\n",
    "entropy , relevant_tokens, logits = get_tokens_entropy(model, tokenized_most_uncertain_sentence)\n",
    "\n",
    "decode_and_print_tokens_with_entropy(relevant_tokens, entropy, should_display_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' nomination', ' he', ' works', ' &', ' power']\n"
     ]
    }
   ],
   "source": [
    "# Get the n keys with the highest entropy\n",
    "top_n_tokens = get_top_n_entropy_tokens(entropy, relevant_tokens, 5)\n",
    "top_n_tokens_decoded = tokenizer.batch_decode(top_n_tokens)\n",
    "\n",
    "print(top_n_tokens_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs of tokens with the highest entropy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[' nomination', ' works'], [' he', ' respective'], [' &', ' actual']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ngram length\n",
    "n = 2\n",
    "# number of pairs to print \n",
    "k = 3\n",
    "decoded = tokenizer.batch_decode(relevant_tokens)\n",
    "print(\"Pairs of tokens with the highest entropy:\")\n",
    "get_k_pairs_of_highest_entropy(entropy, decoded, k, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('power bills', 0.6338), ('respective utilities', 0.4731), ('these values', 0.4639), ('values', 0.4312), ('actual power', 0.4281)]\n"
     ]
    }
   ],
   "source": [
    "# Use keybert to extract keywords from the most uncertain sentence\n",
    "keywords = kw_model.extract_keywords(most_uncertain_sentence, keyphrase_ngram_range=(1, 2), stop_words=None)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "- There is some matching between keybert and the outputs with the highest entropy of GPT2\n",
    "- However we can do better, for the task of keywords extraction bi-directional context could improve results.\n",
    "- Removal of stop-words, punctuation can improve results of key-phrases(but we need to remove them after the calculation of entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Bi-Directional Encoder for Keyword Prediction \n",
    "\n",
    "### Motivation:\n",
    "- Bi-directional encoder has context from both sides which will surely help in terms of keyword identification task.\n",
    "\n",
    "### Action plan:\n",
    "- We'll use a bi-directional encoder model to MASK each token in turn and get the highest cross entropy tokens.\n",
    "- We'll filter out stopwords, determiners, conjunctions and pronouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roberta \n",
    "- Base model is similar size to Bert-Large and GPT2 (335mil params)\n",
    "- Same tokenizer as GPT2(different than bert which uses WordPiece)\n",
    "- Different training scheme than bert(removing next sentence prediction, dynamic masking scheme, bigger batches, longer sequences, longer train time)\n",
    "- https://huggingface.co/roberta-base\n",
    "- Paper: https://arxiv.org/pdf/1907.11692.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token = \"<mask>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_from_text(text):\n",
    "    doc = nlp(text)\n",
    "    words = [token.text for token in doc]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_mask_text_sequences(words):\n",
    "    mask_sequences = []\n",
    "    for i in range(len(words)):\n",
    "        mask_sequence = words.copy()\n",
    "        mask_sequence[i] = mask_token\n",
    "        # Concat the words list into a string\n",
    "        mask_sequence = \" \".join(mask_sequence)\n",
    "        mask_sequences.append({\"mask_sequence\": mask_sequence, \"target\": words[i]})\n",
    "    return mask_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline('fill-mask', model='roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import roberta-base fast tokenizer\n",
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer_roberta = RobertaTokenizerFast.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_mail_in_subwords(tokenizer, email_body):\n",
    "    # Tokenize the email body\n",
    "    tokenized_email = tokenizer(email_body,return_tensors=\"pt\", truncation=True, add_special_tokens=False)\n",
    "    # Get the subwords\n",
    "    subwords = tokenizer.batch_decode(tokenized_email[\"input_ids\"][0])\n",
    "    # Print the subwords\n",
    "    return subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scored_strings(email_body, use_get_words_from_text = False, should_remove_non_verbs_and_nouns = False):\n",
    "    words = get_words_from_text(email_body) if use_get_words_from_text else output_mail_in_subwords(tokenizer_roberta, email_body)\n",
    "    mask_sequences = generate_mask_text_sequences(words)\n",
    "    token_strings = []\n",
    "    entropies = []\n",
    "    target_strings = []\n",
    "\n",
    "    # For each mask sequence, call pipeline with the mask sequence and target word\n",
    "    for mask_sequence in mask_sequences:\n",
    "        target = mask_sequence[\"target\"]\n",
    "        res= unmasker(mask_sequence[\"mask_sequence\"], targets=mask_sequence[\"target\"])[0]\n",
    "        # log2 the score\n",
    "        entropy = -np.log2(res[\"score\"])\n",
    "        token_str = res[\"token_str\"]\n",
    "        token_strings.append(token_str)\n",
    "        entropies.append(entropy)\n",
    "        target_strings.append(mask_sequence[\"target\"])\n",
    "    \n",
    "    return token_strings, entropies, target_strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don: Can you give me an idea of where things stand on the Transfer Agreement? I think that we were just waiting for you to get signed up correctly on Enron Online. I will be going on maternity leave shortly and just wanted to figure out how much I could get done before I leave.\n"
     ]
    }
   ],
   "source": [
    "# Get a random email\n",
    "email_body = sample_email()\n",
    "print(email_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The specified target token ` Can` does not exist in the model vocabulary. Replacing with `ĠCan`.\n",
      "The specified target token ` you` does not exist in the model vocabulary. Replacing with `Ġyou`.\n",
      "The specified target token ` give` does not exist in the model vocabulary. Replacing with `Ġgive`.\n",
      "The specified target token ` me` does not exist in the model vocabulary. Replacing with `Ġme`.\n",
      "The specified target token ` an` does not exist in the model vocabulary. Replacing with `Ġan`.\n",
      "The specified target token ` idea` does not exist in the model vocabulary. Replacing with `Ġidea`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Do the analysis on token level or word level\u001b[39;00m\n\u001b[0;32m      2\u001b[0m use_get_words_from_text \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m token_strings, entropies, target_strings \u001b[39m=\u001b[39m get_scored_strings(email_body, use_get_words_from_text)\n",
      "Cell \u001b[1;32mIn[104], line 11\u001b[0m, in \u001b[0;36mget_scored_strings\u001b[1;34m(email_body, use_get_words_from_text, should_remove_non_verbs_and_nouns)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m mask_sequence \u001b[39min\u001b[39;00m mask_sequences:\n\u001b[0;32m     10\u001b[0m     target \u001b[39m=\u001b[39m mask_sequence[\u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m---> 11\u001b[0m     res\u001b[39m=\u001b[39m unmasker(mask_sequence[\u001b[39m\"\u001b[39;49m\u001b[39mmask_sequence\u001b[39;49m\u001b[39m\"\u001b[39;49m], targets\u001b[39m=\u001b[39;49mmask_sequence[\u001b[39m\"\u001b[39;49m\u001b[39mtarget\u001b[39;49m\u001b[39m\"\u001b[39;49m])[\u001b[39m0\u001b[39m]\n\u001b[0;32m     12\u001b[0m     \u001b[39m# log2 the score\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     entropy \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39mlog2(res[\u001b[39m\"\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\NLP Compression Seminar\\Presentation\\.venv\\lib\\site-packages\\transformers\\pipelines\\fill_mask.py:239\u001b[0m, in \u001b[0;36mFillMaskPipeline.__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    218\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[39m    Fill the masked token in the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[39m        - **token** (`str`) -- The predicted token (to replace the masked one).\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    240\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(inputs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    241\u001b[0m         \u001b[39mreturn\u001b[39;00m outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\NLP Compression Seminar\\Presentation\\.venv\\lib\\site-packages\\transformers\\pipelines\\base.py:1074\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1072\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[0;32m   1073\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1074\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\NLP Compression Seminar\\Presentation\\.venv\\lib\\site-packages\\transformers\\pipelines\\base.py:1081\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1079\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1080\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1081\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1082\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1083\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\NLP Compression Seminar\\Presentation\\.venv\\lib\\site-packages\\transformers\\pipelines\\base.py:990\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m    988\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[0;32m    989\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m--> 990\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m    991\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m    992\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\NLP Compression Seminar\\Presentation\\.venv\\lib\\site-packages\\transformers\\pipelines\\fill_mask.py:101\u001b[0m, in \u001b[0;36mFillMaskPipeline._forward\u001b[1;34m(self, model_inputs)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward\u001b[39m(\u001b[39mself\u001b[39m, model_inputs):\n\u001b[1;32m--> 101\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs)\n\u001b[0;32m    102\u001b[0m     model_outputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model_inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    103\u001b[0m     \u001b[39mreturn\u001b[39;00m model_outputs\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\NLP Compression Seminar\\Presentation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\NLP Compression Seminar\\Presentation\\.venv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1100\u001b[0m, in \u001b[0;36mRobertaForMaskedLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1092\u001b[0m \u001b[39m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[39m    Used to hide legacy arguments that have been deprecated.\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1098\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1100\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[0;32m   1101\u001b[0m     input_ids,\n\u001b[0;32m   1102\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1103\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1104\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1105\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1106\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1107\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1108\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m   1109\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1110\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1111\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1112\u001b[0m )\n\u001b[0;32m   1113\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1114\u001b[0m prediction_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\NLP Compression Seminar\\Presentation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\NLP Compression Seminar\\Presentation\\.venv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:853\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    844\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    846\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m    847\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    848\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    851\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    852\u001b[0m )\n\u001b[1;32m--> 853\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m    854\u001b[0m     embedding_output,\n\u001b[0;32m    855\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    856\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    857\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    858\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m    859\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    860\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    861\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    862\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    863\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    864\u001b[0m )\n\u001b[0;32m    865\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    866\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\NLP Compression Seminar\\Presentation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\NLP Compression Seminar\\Presentation\\.venv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:527\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    518\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    519\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    520\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    524\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    525\u001b[0m     )\n\u001b[0;32m    526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 527\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    528\u001b[0m         hidden_states,\n\u001b[0;32m    529\u001b[0m         attention_mask,\n\u001b[0;32m    530\u001b[0m         layer_head_mask,\n\u001b[0;32m    531\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    532\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    533\u001b[0m         past_key_value,\n\u001b[0;32m    534\u001b[0m         output_attentions,\n\u001b[0;32m    535\u001b[0m     )\n\u001b[0;32m    537\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    538\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\NLP Compression Seminar\\Presentation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\NLP Compression Seminar\\Presentation\\.venv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:412\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    401\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    402\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    409\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    410\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    411\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 412\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    413\u001b[0m         hidden_states,\n\u001b[0;32m    414\u001b[0m         attention_mask,\n\u001b[0;32m    415\u001b[0m         head_mask,\n\u001b[0;32m    416\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    417\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    418\u001b[0m     )\n\u001b[0;32m    419\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    421\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\NLP Compression Seminar\\Presentation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\NLP Compression Seminar\\Presentation\\.venv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:339\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    330\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    331\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    337\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    338\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 339\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    340\u001b[0m         hidden_states,\n\u001b[0;32m    341\u001b[0m         attention_mask,\n\u001b[0;32m    342\u001b[0m         head_mask,\n\u001b[0;32m    343\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    344\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    345\u001b[0m         past_key_value,\n\u001b[0;32m    346\u001b[0m         output_attentions,\n\u001b[0;32m    347\u001b[0m     )\n\u001b[0;32m    348\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    349\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\NLP Compression Seminar\\Presentation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\NLP Compression Seminar\\Presentation\\.venv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:196\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    187\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    188\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    194\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    195\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 196\u001b[0m     mixed_query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery(hidden_states)\n\u001b[0;32m    198\u001b[0m     \u001b[39m# If this is instantiated as a cross-attention module, the keys\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     \u001b[39m# and values come from an encoder; the attention mask needs to be\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     \u001b[39m# such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     is_cross_attention \u001b[39m=\u001b[39m encoder_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\NLP Compression Seminar\\Presentation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\NLP Compression Seminar\\Presentation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Do the analysis on token level or word level\n",
    "use_get_words_from_text = False\n",
    "token_strings, entropies, target_strings = get_scored_strings(email_body, use_get_words_from_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokens_with_entropy(token_strings, entropies):\n",
    "    for i in range(len(token_strings)):\n",
    "        print(f\"({token_strings[i]}, {entropies[i]}, {i})\", end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "# Import punctuation\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "determiners = [\"the\", \"a\", \"an\", \"this\", \"that\", \"these\", \"those\"]\n",
    "conjunctions = [\"and\", \"or\", \"but\", \"because\", \"so\", \"yet\", \"for\"]\n",
    "pronouns = [\"he\", \"she\", \"it\", \"they\", \"we\", \"you\", \"I\", \"me\", \"him\", \"her\", \"them\", \"us\", \"you\"]\n",
    "# Create a set out of the following lists\n",
    "stop_words = set(STOP_WORDS)\n",
    "stop_words.update(determiners)\n",
    "stop_words.update(conjunctions)\n",
    "stop_words.update(pronouns)\n",
    "stop_words.update(punctuation)\n",
    "\n",
    "def remove_stop_words(token_strings, entropies):\n",
    "    indices_to_remove = []\n",
    "    token_strings = token_strings.copy()\n",
    "    # remove spaces from token strings\n",
    "    token_strings = [token_string.replace(\" \", \"\") for token_string in token_strings]\n",
    "    \n",
    "    # check which indices to remove\n",
    "    for i in range(len(token_strings)):\n",
    "        if token_strings[i] in stop_words:\n",
    "            indices_to_remove.append(i)\n",
    "    \n",
    "    filtered_tokens_strings, filtered_entropies = [], []\n",
    "\n",
    "    for i in range(len(token_strings)):\n",
    "        if i not in indices_to_remove:\n",
    "            filtered_tokens_strings.append(token_strings[i])\n",
    "            filtered_entropies.append(entropies[i])\n",
    "\n",
    "    return filtered_tokens_strings, filtered_entropies\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_k_pairs_of_highest_entropy_roberta(token_strings, entropies, number_of_keywords, n_gram_length, should_remove_stop_words = True):\n",
    "\n",
    "    entropies, token_strings = entropies.copy(), token_strings.copy()\n",
    "    if should_remove_stop_words:\n",
    "        token_strings, entropies = remove_stop_words(token_strings, entropies)\n",
    "    \n",
    "    entropy_pairs = []\n",
    "    entropy_pairs_tokens = []\n",
    "    temp = len(entropies) - n_gram_length\n",
    "    # Fill tensor with average entropy of each k pair\n",
    "    for i in range(len(entropies) - n_gram_length + 1):\n",
    "        entropy_pairs.append(np.mean(entropies[i:i+n_gram_length]))\n",
    "        entropy_pairs_tokens.append(token_strings[i:i+n_gram_length])\n",
    "    \n",
    "    if len(entropy_pairs) != len(entropy_pairs_tokens):\n",
    "        print(\"Here\")\n",
    "\n",
    "    # Choose the n pairs with the highest average entropy\n",
    "    top_n_pairs = get_top_k_indices(entropy_pairs, number_of_keywords)\n",
    "\n",
    "    # Get original \n",
    "    top_n_pairs_tokens = [entropy_pairs_tokens[i] for i in top_n_pairs]\n",
    "\n",
    "    print(top_n_pairs_tokens)\n",
    "\n",
    "    return top_n_pairs_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(On, 0.4832911287476545, 0) ( October, 5.93962416933179, 1) ( 29, 4.353486444205462, 2) (,, 2.1068894067495263, 3) ( 2001, 1.949119249034914, 4) (,, 2.5258204900919448, 5) ( the, 0.19069954673731446, 6) ( commission, 4.178079396682821, 7) ( issues, 5.799599587146871, 8) ( an, 0.18728045915157995, 9) ( order, 3.4605743260892647, 10) ( instit, 19.833574232008278, 11) (uting, 0.21538432303071822, 12) ( a, 7.60285536167855, 13) ( rule, 13.20810969721396, 14) (making, 10.783790267685058, 15) ( to, 1.8517606818668912, 16) ( establish, 10.13264584031978, 17) ( rat, 27.20614113027872, 18) (em, 17.597692589401593, 19) (aking, 11.792045539681405, 20) ( mechanisms, 9.823738265750125, 21) ( to, 0.5982147661842263, 22) ( enable, 7.03358300197307, 23) ( S, 10.62558358702089, 24) (CE, 4.551327233069364, 25) (,, 1.249330832487079, 26) ( PG, 6.703664737864915, 27) (&, 3.867114840484336, 28) (E, 4.54962200428811, 29) ( and, 3.1251497563370174, 30) ( SD, 23.441029572519355, 31) (G, 0.4086155350426766, 32) (&, 4.658974233925184, 33) (E, 9.079529015621075, 34) ( to, 0.21233433230618332, 35) ( resume, 8.989692838460524, 36) ( purchasing, 7.949499159456826, 37) ( electric, 7.622959896122297, 38) ( energy, 6.57690384306614, 39) (,, 6.70410091519906, 40) ( capacity, 11.90110385380896, 41) (,, 13.077835655483634, 42) (anc, 28.410607384987305, 43) (illary, 0.006093037132654717, 44) ( services, 6.537544033529991, 45) ( and, 3.3073726867307944, 46) ( related, 10.322416479283994, 47) ( hed, 27.688955604831484, 48) (ging, 1.3698596576905582, 49) ( instrument, 20.31546880543685, 50) ( to, 3.159294289169838, 51) ( full, 15.721774454533374, 52) ( their, 3.4208847039129924, 53) ( obligation, 5.577832806000423, 54) ( to, 0.00924706785591683, 55) ( serve, 7.333678354149635, 56) ( their, 9.230303475112203, 57) ( customers, 0.7700794270924696, 58) ( once, 8.21812325544431, 59) ( D, 0.08513105645219497, 60) (WR, 3.7814268608762722, 61) ('s, 0.47771696516634016, 62) ( authority, 6.512905193210337, 63) ( to, 0.13287277207185863, 64) ( purchase, 6.505300030657192, 65) ( on, 16.3381474377852, 66) ( their, 1.5676951415416647, 67) ( behalf, 4.7990716196216985, 68) ( ends, 4.490201321240454, 69) ( (, 0.6845007478718915, 70) (December, 9.4306400185522, 71) ( 31, 3.7211943949947095, 72) (,, 2.493772333629161, 73) ( 2001, 0.9602174585511042, 74) ()., 6.127186868079095, 75) ( At, 0.3056431756047362, 76) ( that, 1.5055215823562067, 77) ( point, 3.4111293894990005, 78) (,, 1.7218904660672518, 79) ( the, 3.1846030219911703, 80) ( U, 20.677492371201808, 81) (DC, 14.377528415378867, 82) (s, 6.996753184155031, 83) (,, 2.7321287593926913, 84) ( not, 7.295544973290012, 85) ( D, 0.32240723211629346, 86) (WR, 3.465493589797223, 87) (,, 9.282791955586845, 88) ( will, 0.3827390390145767, 89) ( have, 0.37420310450541255, 90) ( to, 0.009543512413962545, 91) ( pick, 1.7362734727153608, 92) ( up, 3.7908730106899076, 93) ( anything, 12.43658596348626, 94) ( not, 4.415788915148237, 95) ( covered, 0.957918345924836, 96) ( by, 0.49180384563870294, 97) ( retained, 17.41304513809339, 98) ( generation, 10.684437591584219, 99) ( and, 6.557668338599464, 100) ( the, 3.4147592581825443, 101) ( D, 3.7200300174871384, 102) (WR, 3.2145146050912112, 103) ( contracts, 14.57473465860187, 104) (., 3.3925463014713935, 105) ( The, 3.4924270870449305, 106) ( Commission, 5.883128439175001, 107) ( will, 1.9497134013154716, 108) ( also, 0.484889120439491, 109) ( address, 10.24092170426084, 110) ( proposals, 15.736656287864054, 111) ( on, 2.0874632927048684, 112) ( how, 2.2783589150512755, 113) ( renewable, 14.463059064942817, 114) ( resources, 5.341728579985256, 115) ( should, 2.873186368144809, 116) ( be, 0.009371783530479955, 117) ( included, 3.7124023079936883, 118) ( in, 0.1505562412260204, 119) ( the, 0.08081477891166437, 120) ( mix, 4.976770520214751, 121) ( of, 4.1328213863926955, 122) ( generation, 11.950544263072716, 123) ( serving, 13.24158035235746, 124) ( the, 0.2396917118371712, 125) ( state, 2.5634638163722188, 126) (., 3.545987306239265, 127) ( The, 2.1006582908684623, 128) ( O, 19.598316047672114, 129) (IR, 12.599751546257302, 130) ( will, 0.6315612133012718, 131) ( have, 0.07060332887621319, 132) ( a, 5.0256346903910085, 133) ( significant, 2.5241190347869558, 134) ( impact, 2.384971889714578, 135) ( on, 0.04928953645893073, 136) ( procurement, 6.270239972213783, 137) ( practices, 13.593216555343432, 138) ( in, 2.1330245826585377, 139) ( the, 0.2135134918478429, 140) ( state, 0.3412356632098265, 141) ( on, 0.18151563234915416, 142) ( a, 6.400474266450714, 143) ( going, 0.8619627721015964, 144) ( forward, 0.14012052233122468, 145) ( basis, 5.266312056789298, 146) (., 2.7166289937123373, 147) ( The, 0.3147899931930077, 148) ( ruling, 8.505978220445298, 149) ( states, 3.295008289943108, 150) ( that, 1.2637237049582266, 151) ( if, 0.05336317973668552, 152) ( you, 1.8177332639586317, 153) ( are, 0.1956831294833257, 154) ( interested, 0.36487043591292384, 155) ( in, 0.015500963889692266, 156) ( being, 6.973345029561653, 157) ( on, 0.2124686346997162, 158) ( the, 0.39039604424360225, 159) ( service, 12.05850197650644, 160) ( list, 7.270659525043031, 161) ( for, 1.5473876056579052, 162) ( this, 0.3157302520583883, 163) ( proceeding, 4.916962070809489, 164) (,, 2.3300058828067542, 165) ( a, 0.19615188607994372, 166) ( letter, 0.06348165918014587, 167) ( must, 1.0856139553565842, 168) ( be, 0.007852994263159831, 169) ( sent, 0.633345482668464, 170) ( to, 0.024236766764187408, 171) ( the, 0.7258973074654661, 172) ( d, 17.232762881553068, 173) (ocket, 16.401127740806004, 174) ( office, 7.712045735711004, 175) ( stating, 7.20857913103096, 176) ( that, 1.8874682252241277, 177) ( fact, 21.17758574158636, 178) ( within, 1.6514082566171409, 179) ( 15, 4.136353813610026, 180) ( days, 0.24867956920680023, 181) ( (, 6.371307058991236, 182) (by, 8.906753478207403, 183) ( next, 3.9423253435430654, 184) ( Monday, 4.409375261616563, 185) ()., 5.007405370328825, 186) ( Bottom, 4.282627723339062, 187) ( line, 8.781153448516859, 188) ( is, 6.419084648287237, 189) ( do, 16.009147978076893, 190) ( you, 1.543530271766652, 191) ( want, 0.44682833718427867, 192) ( me, 11.699433218384629, 193) ( to, 0.16287382765373412, 194) ( continue, 2.4214019189676876, 195) ( to, 2.7593520374251796, 196) ( monitor, 9.660553231753804, 197) ( (, 11.125745313480076, 198) (and, 1.0739692809408679, 199) ( perhaps, 6.727275969940954, 200) ( get, 11.76586685728772, 201) ( active, 12.313788886865241, 202) (), 3.6848391025926035, 203) ( in, 2.2230696472950293, 204) ( this, 0.16311740771409894, 205) ( proceeding, 3.385059254311176, 206) (., 4.89953110651604, 207) ( Please, 4.031964582373323, 208) ( let, 0.012932411924535517, 209) ( me, 0.055000347711966074, 210) ( know, 0.08298692410350429, 211) ( by, 1.5120673763716566, 212) ( the, 0.4731466735023707, 213) ( end, 0.08404697031676386, 214) ( of, 0.16140811322865656, 215) ( the, 2.055639129958279, 216) ( week, 9.803249309220053, 217) ( so, 14.354959514844536, 218) ( that, 1.2243732435339374, 219) ( I, 0.21838244811130905, 220) ( can, 1.2928109844249227, 221) ( send, 1.2600520437158782, 222) ( in, 14.507787267804993, 223) ( the, 0.2613743528373557, 224) ( required, 4.157018157619562, 225) ( letter, 6.474474650756775, 226) (., 2.611261894201305, 227) ( Jeanne, 26.280544538645927, 228) ( Bennett, 21.50072707142985, 229) "
     ]
    }
   ],
   "source": [
    "\n",
    "print_tokens_with_entropy(target_strings if use_get_words_from_text else token_strings, entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On October 29, 2001, the commission issues an order instituting a rulemaking to establish ratemaking mechanisms to enable SCE, PG&E and SDG&E to resume purchasing electric energy, capacity ,ancillary services and related hedging instrument to full their obligation to serve their customers once DWR's authority to purchase on their behalf ends (December 31, 2001). At that point, the UDCs, not DWR, will have to pick up anything not covered by retained generation and the DWR contracts. The Commission will also address proposals on how renewable resources should be included in the mix of generation serving the state. The OIR will have a significant impact on procurement practices in the state on a going forward basis. The ruling states that if you are interested in being on the service list for this proceeding, a letter must be sent to the docket office stating that fact within 15 days (by next Monday). Bottom line is do you want me to continue to monitor (and perhaps get active) in this proceeding. Please let me know by the end of the week so that I can send in the required letter. Jeanne Bennett\n"
     ]
    }
   ],
   "source": [
    "print(email_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With stop words filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['rat', 'em', 'aking'], ['establish', 'rat', 'em'], ['letter', 'Jeanne', 'Bennett'], ['hed', 'ging', 'instrument']]\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "k = 4\n",
    "\n",
    "top_n_pairs_tokens = print_k_pairs_of_highest_entropy_roberta(target_strings if use_get_words_from_text else token_strings, entropies, k, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without stop words filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' rat', 'em', 'aking'], [' establish', ' rat', 'em'], [' capacity', ',', 'anc'], ['.', ' Jeanne', ' Bennett']]\n"
     ]
    }
   ],
   "source": [
    "top_n_pairs_tokens = print_k_pairs_of_highest_entropy_roberta(target_strings if use_get_words_from_text else token_strings, entropies, k, n, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison to GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs of tokens with the highest entropy:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[' capacity', ',', 'anc'],\n",
       " [' to', ' full', ' their'],\n",
       " [' full', ' their', ' obligation'],\n",
       " [',', ' capacity', ',']]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GP2 to extract keywords from the email body\n",
    "# Tokenize the email body\n",
    "tokenized_email_body = tokenizer(tokenizer.bos_token + email_body, return_tensors=\"pt\", add_special_tokens = True)\n",
    "\n",
    "# Get the entropy of the tokens\n",
    "email_body_tokens_entropy, relevant, _ = get_tokens_entropy(model, tokenized_email_body)\n",
    "\n",
    "decoded = tokenizer.batch_decode(relevant)\n",
    "print(\"Pairs of tokens with the highest entropy:\")\n",
    "get_k_pairs_of_highest_entropy(email_body_tokens_entropy, decoded, k, n)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison to Keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dwr authority purchase', 0.5601), ('generation dwr contracts', 0.539), ('dwr contracts commission', 0.5337), ('dwr contracts', 0.5121), ('sdg resume purchasing', 0.5116)]\n"
     ]
    }
   ],
   "source": [
    "# Use keybert to extract keywords from the email body\n",
    "keywords = kw_model.extract_keywords(email_body, keyphrase_ngram_range=(1, n))\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "- Entropy isn't always a good indicator of saliency as humans conceive it- high entropy might be in names or specific phrases\n",
    "- Sentence embedding with cosine similarity on keywords might be a better option for some cases(embedding space is noun centric)\n",
    "- A bi-directional model works better for the task, since it has context from both sides.\n",
    "- Entropy comparison can be used for other purposes - such as evaluation of a model memorizing training data(Collin Raffel example) - https://arxiv.org/pdf/2012.07805.pdf\n",
    "This paper showcases that LLMs are prone to memoizing data even if it appeared once during a training loop.\n",
    "Which is important identification of PII data being exposed by the model. By comparing the entropy with an unbiased source such as zlib or other LMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges:\n",
    "\n",
    "- Tried to fine-tune the models on emails - can't due to compute (could have used distilled gpt but I think it would have hurt the perplexity more)\n",
    "- Conditional entropy is calculated on tokenized words which can be subwords - which makes it harder to get actual words back.\n",
    "- To calculate the entropy/conditional you can't mutate the sentence before the calculation, only after it's calculated you can identify stopwords/nouns/verbs from tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work:\n",
    "\n",
    "- Do better filtering after tokenization(leave only verbs and nouns)\n",
    "- Evaluate method on existing benchmarks(semeval 2010 task 5, semeval 2017 task 10) - https://aclanthology.org/S10-1004/, https://paperswithcode.com/dataset/semeval2017"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "64e08e2444c796b702b473c4ffb135831e2a608e80569b068135e05fb196445e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
